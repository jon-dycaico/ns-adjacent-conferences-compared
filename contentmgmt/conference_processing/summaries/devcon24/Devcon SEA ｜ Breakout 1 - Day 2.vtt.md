[Music]
he
o
B o
oh
you
B
a
I'm
I'm I'm I'm
h
no
oo
pap
got
up
come
n
look
o oh
I
m
C
oh oh
b b
BL
to o
t
the
oo o
all right welcome to EPF day thanks for
being
here uh my name is Josh this is Mario we
uh coordinate the uh ethereum protocol
Fellowship uh with the ethereum
foundation uh and this is EPF day where
the fellows from the fifth cohort will
be giving you lots and lots of
demonstrations about what they have been
working on over the past five months
uh though quickly we want to talk a
little bit about what the ethereum
protocol Fellowship is um this is a
program that's been running for about uh
three years three years now uh it's been
going through five different cohorts uh
starting with uh what was called the
core developer Apprentice program
started by Piper Mariam uh and then
Mario and I have been running this
iteration of the program for the past
three cohorts
uh the protocol Fellowship is a program
that is meant to Steward the protocol
through the uh bringing in of more and
more developers so um according to the
latest protocol Guild statistics there
are about 180 uh developers that are
working on the core protocol uh all the
things from specification to working on
El clients C clients doing testing and
also doing lots of the research that is
necessary for uh future upgrades and
forks uh so it's a lot of stuff that
needs to get done and not that many
people to do
it uh as Tim beo likes to say and I
think others uh I'm sure you've seen
this road map before uh that each one of
these tiny little boxes could use an
entire team uh to focus on just that box
so um we are here to try to fill in all
those
boxes uh the EPF the protocol Fellowship
has sort of grown a lot over the past
couple of years and we've been adding
new and new features to it um to enhance
the experience and to make it easier for
people who are interested in uh doing
this work to do so so it started with
the EPF again this is the fifth cohort
uh at the beginning of this year we
started the ethereum protocol study
group and that was a thre Monon two
Monon long program uh that offers um
some
more basic introduction and overview to
the different pieces of the protocol so
those people that are interested and
maybe have some developer experience but
don't really have much uh knowledge of
ethereum itself can get introduced to it
and learn about the different pieces and
parts and understand what it is that
they might want to focus on uh in
addition to that we've created a
resource uh EPF Wiki which is uh the
intention is for it to be the uh a go-to
place for people who are interested in
learning about the protocol um to to do
so answering questions and and helping
them to understand more about it uh it
is a growing and and collaborative
resource so uh it is in it's still its
sort of fledgling form and uh we welcome
any further contributions to it if you
are learning about the protocol or
already are very knowledgeable about it
to to add your two cents to
it uh and finally uh like Piper likes to
say the door to core development is
hilariously wide open you just have to
step through the EPF is a way for uh you
to step through with somebody holding
your hand a little
bit uh I'll go ahead and pass it to mark
Mario yeah thank you so much and let me
tell you a bit about the current cohort
so as Josh mentioned we started this
cohort like I feel like this cour
started even before it started because
we did a study group first and it was a
very special experience we started with
that uh this time for the first time and
there have been a bunch of people who
came to learn about the protocol
basically from zero and then uh joined
uh the cohort and in last uh six months
or this during this year we started the
the the study group like February March
so like during this year people who
didn't have any idea about the core
development became like uh almost
full-time contributors in some cases so
yeah uh it's been it's been a wi wild
right we had uh uh we had around 45
follows uh just some context to this
number because we started when when we
when we start the the cohort it's
permissionless anybody can join and in
his first calls we had like over 50
people and we like 60 uh people who were
interested in the fellowship but then uh
at least 45 of them um uh submitted
multiple updates throughout multiple
weeks uh during like two months or
something so so 45 of them at least made
it to the second month but to the very
end till today uh we have 30 fellows who
made it to the EPF day uh there are few
of them who couldn't make it of course
because of the travel restrictions and
so on but uh uh we had
uh anyway um okay put it further okay
five months of the the echo 5 months of
the of the uh actual Fellowship uh which
was first some learning part uh first
month to uh figure out what projects
these people want to even work on and
for many it was easier thanks to the uh
thanks to the study group uh they
than ever before uh we we had two weeks
uh two two calls every week uh so all
together we had over 40 calls together
over past five months we've met each
other every week for the office hours
for the standup so uh ton of call
but also we met in person uh first time
we met in person in Brussels we had a
chance to meet with maybe half of the
fellows but almost everyone is here
today at Devcon so it's really honored
to actually meet you finally all after
seeing each other at these calls yeah uh
so 30 people made it till the end with
uh 30 projects it's not one project per
person there were teams of people
working uh on a bigger scope project
together which is also amazing to see
that people learn to collaborate
together and uh uh we will have
presentation for 20 projects today um 30
were proposed uh yeah and uh uh Al
together we had like over 500 comments
over 500 updates in the in the
repository so when you open the develop
and updates in the cord 5 repository
with all of the tracking for every
follow it's over 500 of them so I guess
not I read all of them but it's not
possible to to to do it by yourself but
uh uh there's been there's been lot of
lot of uh lot of work done uh and it's
hard to summarize like what individual
fellow did I I know that Rahul
summarized it so I kind of borrowed his
his data here uh just to give you idea
what is like the output of one fellow
who dedicated like thousand hours to uh
to um uh the fellowship uh and uh how
many
caes 228 cups of coffee yeah uh that's
uh that's that's that's interesting uh
metric that I would like to also use for
measuring the EPF but yeah thank you so
much for the numbers Rahul like so this
is and 26 updates just from just from
one person so you see like this is what
the single person is able to dedicate to
the fellowship over five months and um
so with this today again it's an honor
to host the project presentations to see
uh your recap of what you've been
working on in past months and there is
have ton of topics to go over I'm not
going to even mention each each of them
because you will see them uh this is the
order and we are starting with evm uh
memory repricing so maybe let's get
slowly ready to that and um yeah we will
have uh uh we will have uh first half of
the presentations in the in the morning
with a pause at 1 1 p.m.
1:15 5 because
we have yeah uh quarter past 1 uh we
will have uh 45 minutes for lunch and
then we meet here again at 2 p.m.
for
the rest of the presentations and uh for
the panel at two we will have a panel
discussion as well so that's the
schedule for today uh please uh enjoy
the EPF day uh and uh yeah uh I hope you
get inspired by the Fells by their work
I hope you are able to meet some
interesting people here uh meet uh with
mentors and with with other people that
you might work in the future yeah thank
you so much for being here uh and uh
yeah let's
start um so yeah let's not if you if we
have the first presentation ready let's
not even hesitate and and get into it uh
our first Speaker today is uh Rahul who
who was working on uh benchmarking evm
on figuring out whether the gas pricing
that we have uh is sound and um uh he
got a bunch of interesting data uh so
thank you so much for being here
ra
uh right oh yeah wait is this one yeah
yeah yeah nice okay thank you so much
Rahul go ahead it's it's yours yeah
thank you hi um I'm Rahul I'm going to
talk
[Applause]
about okay so I'm going to talk about
evm memory um I understand that evm is
kind of like an esoteric topic but uh
for the scope of this this discussion uh
evm is really just a
computer um nothing to be scared of um
like any computer it has like um
memory unit which is like a fundamental
unit to store ifal
data um like to give you some stat 30
years ago memory cost about for 1
Megabyte it cost about $32 that's uh in
penny that's
idea
e
e e
both story told do where is the secer
hi everyone uh excuse
me that's have a look oh okay uh hi
everyone this is J visa and I come
back State DV and to rep transitions
this is really slow and it cost a lot of
CPO um so we want to introduce a low
time in tracing system which will offer
more efficient and promote insights into
the EM execution and it will also
support indexing the LIF execution data
uh such as the ches the state differ and
the as
s um so let's see how can we go to uh
how to index Source left
data um currently we uh support index in
ches uh which is a uh which is uh
grouped by a block and so say is a block
number and minus two the ches and us us
indexing is based on the n n this is uh
when we want to index is trans Cent
adress plus Isn to our transition H um
so uh in in my explation I'm using uh
kbdb us to do newly created uh block
chases and noes uh this is just this is
just uh genical KV database and I use
the EB in gas um for the king schema for
the traces it is just a block number
plus Block H has and chest Tab and for n
it is uh
used the king schema is transing senders
adj plus is n and this value just is
just I encoded data so the is very is
very small and very very simple and uh
for Chas tapers currently we support the
Cod Chas and other chases such as parity
Chas and this pred
Chas uh here is the uh is a workflow of
indexing and
we uh for each for each block uh we for
each block we will indexing each
transations based on the on on the Tas
St hook and in hook we will to see if we
non if if so we we first NES in kbdb and
if not we just continue and for each
transition end we get uh transition
result in in our local Chase in our
local catch and when the block is ended
we we St the block chases into the kbdb
and in the meantime we will notify our
us background foron in uh to notify him
uh we have just uh indexed a new block
and fer will for this the newly created
traces into rdb for the
infections and um so this is just for
the indexing and let's see how can we
retrieve those
data um for the chases we can retrieve
it by block number and the or block he
uh and for block for the transes
for transitions we can Fe the block
first by the transition has and then
retrieve it by block number and fish uh
fish transes data by its
index uh and for n is is really simple
we can just retrieve it by the CNS
address and
N uh and here is uh just uh a small
follow of the block chases re index yeah
it's really uh it's it's very very
simple just read the block number first
and uh check the uh block data in it's
it's wer in the KV DB or in the for DB
and the then return back to the
column so uh how can we use this feature
uh as currently this feature is not
merged into the main into Master uh
Master bran so if uh if you are
interested you can follow s PR and and
the say um how to use it uh the left
side is uh is the configuration of the
uh to enable feature you just need to
add s uh something like this in the left
side and on right side I will I will I
will I will introduce every
configurations so first one pass is just
a dictionary which is used to store
Source data and the second one in N ches
is uh IND index the left ch way to
enable n Chas or not and the uh the
third one M blocks is just um uh a block
limit so uh after after such blocks way
well PS all the datas just to set dis
dis size and for config is it's um it's
a map it's a map to config each tras and
course each details we can uh you can
refer to the built in G build interest
is for more
detail okay um currently we support
those rpcs uh it's it's like par CH RPC
and the the last one is uh the RPC just
says is just proposal and it is not
enable for other clents I
think um besides of the parties C traces
we can also retrieve other gases native
traces I think is really useful uh when
you want to uh get history um history
data or history Poli data for one Trans
on one
block uh we can just like this just RBC
uh in this example we can we need to
retrieve the TR for the block one
yeah so next we can just take talk about
perform performance yeah uh before I
talk about per performance uh in my my
in my mind performance is just as space
on the time trade offs in in this
impementation we just use uh low cost
disk to store the data instead of the uh
High usage of the CPU so for uh say uh
the the advantage of the feature is just
you no you no need to reapply the chases
based on this data and the time
complexation is very small it's just all
one and the retrieving is real is real
time you can you can IND the data and
then retrieve it back and the this
Advantage you need another disk space to
store this data and it may have a little
impact on the on train block
thinking yeah um here I just write some
bmark to compare the results of uh Chas
IPC and IPC yeah uh it's a just it's a
scrip to
run uh to retrieve the data from the uh
uh gas uh wise J IBC and uh this is our
results of the uh just results uh the
uper one is uh traditional CH block and
lat one is the Cho block it is about 100
times uh
of first T than traditional W yeah uh
say it's all
San thank you very much your question
hey thanks thanks for this really
appreciate it I think as a developer
it's sometimes confusing because there's
no standardization on the dbg or Trace
name spaces right the other RPC methods
are in the execution API so they tend to
be standardized across clients when you
move to debug tracing it becomes a bit
trickier other than then performance
because some clients are going to
implement debug some Implement Trace
name space some do both other than
performance like why would you recommend
Trace over debug is there other reasons
you would recommend that option for
Tracy uh it is more it's more custom
customized you can you can just uh index
data by your need yeah and because the
tradition the B tracing which is also
support only support uh such as the par
Trace C trace and the for C tracing and
uh if you want to index other you might
need to wrate a GS GS traser yeah yeah
and the performance is really very uh is
some some p is not very good so in sit
we just um introduced this u a leing CH
framework and the best on set you can
just write your own ches right thank you
I have another question unless someone
else
maybe
Sorry probably miss this will be
integrated into PR or when when it's
going to be added together um actually
um I'm not sure yeah um maybe this
feature will be U mer into the master
branch and in in 4 or maybe
one no no not need to hard for yeah yeah
just
attention maybe can you touch real quick
on the um you were talking about some
potential issues on the sinking side you
had a con your second bullet point on
the slide where you had pros and cons S
I come back do you remember there was
like two cons you mentioned one was like
additional storage space uhuh for Trace
was one of the disadvantages and then
the second one was you said potential
sinking issues yeah yeah yeah can you
touch on that or go a bit more and just
explain what you meant by that um or
like at least I guess what you observed
when you were benchmarking uh actually
say is not inside impation and I will I
will I will I will attend it and maybe
in other works yeah thank
you thanks next uh so mostly the speed
Improvement looks like it comes from
caching and index in the data did you
look at index in the built-in uh Native
debug Trace uh uh your means the debug
yeah they pre-state in the co Tracer so
that you know you you're basically
saying half of the argument is it's
faster mhm but most of that I suspect
comes from the the fact that you're
indexing all the data did you look at
indexing the debug Trace calls as well
um I'm sorry I
haven't so most of the performance
increase comes
from no oh really yeah yeah not from the
index no not from index in the data yeah
yeah yeah yeah yeah actually the
performance is really depends on the uh
history data yeah yeah you need to feat
also history data needed to replace a
transition so performance is really yeah
yeah because the this uh Fe is really
slow
yeah sorry I I guess like P backing on
that question like the improvements that
you shipped with trace the train name
space could you not backport them to
debug as well or is it is that not
possible yeah because in in the left trm
uh the data is just uh generated and St
on when the trans is once and when you
use the de the data is just replied when
you time when your request it so in this
ination we just start it once and feates
at any time that's fa but you're talking
specifically about the life tracing here
the real- time tracing or you also
speaking about when you're cuz when you
had that graph that was showing 100
milliseconds for Trace block right and
then you had debug Trace block at the
top that wasn't for Life tracing right
that was just IAL calls or where you can
trace any block correct uh no no no
actually for you need to index s St for
okay yeah just after the guest is
started yeah yeah we got to we got a uh
cut it up here I'm sorry guys uh thank
you so much again for the presentation
thank you for all the questions I really
appreciate the discussion but we are
running out of time many presentations
so we got to move on uh s if you're
ready so yeah thank you so much J Visa
it was it was great another Applause for
for J Visa yeah there we go and um yeah
and can I ask you guys for another set
of slides yeah awesome
yeah yeah lot of cool projects lot of
lot of lot of great
cents and another one uh from Sid here
so Sid is one of those people who also
been part of the study group in the
beginning and actually has been working
on this since the beginning of the Year
almost since like February because the
study group introducing the etherum
protocol starts with the introduction um
to uh to the execution layer right and
that's where we started and that's uh
which where s still is basically he
didn't move on from that and uh he's
been working on the uh C nethermind evm
implementation so I'm very curious about
your work yeah go ahead man cheers um
this is next this is before yeah yeah
s
Everyone hi
everyone right so this is my project uh
EPF uh nethermind I evm I evm is a evm
optimization project from
nethermind uh in seha this is sort of
the lose timeline of uh of the project
so week six and eight we sort of done on
some warm-up tasks to kind of get a feel
of the code
base uh week 9 and 12 was done uh was
spent on doing some research for the
upcoming task which is basically
Gathering engr stats the core focus of
the project was doing the stat analyzer
implementation which happened between
week 13 and 16 and then week 17 and 18 I
was basically running the stat analyzer
getting the Ms and then actually doing
the MGR implementations uh for patent
detection mode for
theia and we 19 class I was just
basically doing bu fixing also just
generally looking at IBM and bugs and I
found like five off codes that had
problems and I fixed
those um
so right so the first part was basically
it was um it was my first task um I just
joined
I I just started the project and uh I
think it was the first meeting with the
mentor and I basically was uh the mentor
asked me do you know where I was because
the mentor was sort of focusing on eof
implementation so nobody was working on
evm I just sort of took the task that
was sort of remaining and I started
working on it and then my mentor joined
in and that was sort of finished the
next task that I did was some code DB
stats again this is a warm-up task where
I just Tred to get some engr stats from
the database so these are not execution
stats uh the implementation was very
slow it was done over the weekend again
it was a warmup
task so now we kind of come to the you
know
the oh well this is the research and
algorithm so there was a lot of research
and uh literature review that was done
for basically Big Data analysis because
we dealing with the execution stats for
mgram which is actually like a lot of
data and these were some of the papers
that were run uh that that I that I read
uh of not are basically the heavy
Keepers and also sliding sketches uh in
time Z for data stream processing
because these are both single pass
algorithms because we wanted to
implement an algorithm that ising single
pass for for this amount of data and and
is
efficient so uh after a while we just
sort of U with some discussions with the
mentors we just settled on simple cman
sketch uh the basic principle of the
cman sketch is that you have um sort of
DH functions and then you have like
let's say w buckets and whenever an item
comes that gets hashed and it's put into
one of these buckets so for example here
here you can see just data for one item
so in this case the true Count will be
the lowest count that is one 25 would be
an overestimation that means there's a
collision so but because we have many
hash functions if you take just the
minimum of the count you can actually
get the true
Count adjusted with errors Etc uh for
the data that we need
so uh additionally uh you have you have
these bounds of Epsilon and Delta uh
that that control the probability and
the error uh that the data structure has
and you can actually configure the stats
analyzer to use both width uh depth uh
Epsilon and uh and
Delta so now for the main thing which is
building the stat
analyzer uh okay the first part was
encoding the
engrs so to encode the uh the task was
given is that basically the uh we just
need to find uh 2 to7 um op code
patterns like 2 to7 the size of the
mgram is 2 to S and the way to basically
do it and this could work for like up to
n n gs actually uh it's only limited by
the data type you have like used uh one
of the data types that is common in
clients is like you have a 32 by data
type like un 256 uh so technically it
could go up to like uh I don't know 32
uh 32 op what we can do is we
just uh get the engram the long value
that we have we we find the engr that is
the maximum and then we can
actually uh select that out and
basically if you have like uh an engram
like pop pop ad data you can actually
just it saved anywhere uh and they just
basically you have these sort of
ephemeral new sketch is provisioned um
based on uh what our error threshold is
like what is the Maxis like what is the
max error we able to tolerate so we can
configure that and then new sketch as a
provision based on
that uh it provides the topk
patterns and uh provides the error and
confidence for the stats so you can
actually have very performances for this
so you can actually make it really fast
and less accurate or slower and more
accurate you have that capability
uh right so then how do we gather the
data the the data is done as a trace
it's a trace plug-in uh it's straight
from the evm you can configure this to
be enabled or disabled the Tracer then
finally dumps the data it it calls stats
analyzer and then it can dump the data
into a file as a trace on file so how
does that look this is sort of the
output you can see initial b block block
number current block number error per
item confidence and these are the stats
for example you have the pattern you
have the you know the bites that it is
and then you have the count that you've
observed and you can go on you can
specify how many you want to give an
idea of the
configuration that you can see all these
are like this is the config that you can
you can do you have enabled the file to
write to the write frequencies like how
many blocks you want to write this to
ignore set like you want to ignore jump
destination that's not really useful for
uh for analysis so you can actually
write ignore set you have an instruction
Q size the size of the que used to
gather instructions for Block right
because that's again it's it can
increase as time goes so you want that
to be configurable you have the
processing Q size how many
blocks uh are now you know are stored
for processing uh you have the number of
buckets that you can put in the sketch
you have the number of hash functions
that you can put in the sketch you can
put the max error that you're willing to
tolerate on the CM sketch you have the
sketch minimum confidence that you want
to tolerate you can put the analyzer top
engrs to track like 10 10,000 100,000
you can put uh analyzer Min Supple
threshold this is uh this is sort of
like a filter uh both of these analyzer
capacity and threshold then you have the
sketch buffer
size and uh you have the sketch re reset
and reuse threshold which is like where
gets provisioned a new sketch for uh
based on
error uh so that's the plug-in config
the next section was patent Discovery
selection and
implementation so
uh right so wait right so I used the
stat analyzer I did two sets one was 10
U Top 10 patterns of two size two and
that got merged then I did 11 patterns
of 5 to8 of course still under review
but these is basically in the patent
matching mode of ilm so the the op code
implementation has to be uh in parity
with the evm
implementation but you have certain
opportunities of optimizing because you
have a few patterns together but the
major optimization will come in the I
implementation because we have two
implementations to do one is the patent
matching and the I
implementations uh
the last was testing that was just
started like a week week back so it's
still like very much a work in
progress uh so this involved testing
ilel testing um the patent
implementations also the AIS
implementations and the way we were
doing this was basically we have two
chains and we have an enhanced chain
where we enable the ilm and a normal
chain but we don't and then we compare
the state right but doing that also
there are uh caveats because what if
there's an out of gas error well both
those Chains would be the same what if
there's a you know spec is not enabled
again you will not you'll get the state
route passing the comparison in valid
jump destination so all these uh
situations were there where it would
just pass the test but it's like
implementation is incorrect so it's
quite hard to test
this uh so anyway so there was some
detection that happened over the weekend
um I think five op codes uh that I fixed
while testing and uh testing for the
analyzer op codes and patterns are still
you know work in
progress and well uh thanks to all these
uh I mean my main mentors were Shimon
and
Iman uh and a big thank you to lucash
Dan Adams Damian um for being sort of
weekly there in the ilm calls and Marik
and Thomas for actually enabling at the
beginning to help me you know meet all
my mentors and setting the meetings up
so
yeah thanks thank you so much lovely
thank you yeah uh yes so it's very hard
to fit all that work into few minute
presentation but we still have some
minutes to go for questions so anyone
want to ask a question about ilvm in uh
in CP in under mind any any question for
Sid
Sid oh yeah you're good okay okay go for
you you're free you're chill okay
another another Applause please like
thank you so much Sid awesome and uh
next up is Derek so if you can come to
get ready so yeah thank you so much Sid
and the next up um another presentation
by Derek and uh yeah my honor to welcome
Derek here another of our fellows who's
been working on Port
uh client implementation in um You
music oh no we don't okay that's good
good for us uh yeah so sambba uh Java
implementation of portal and U actually
Derek has been working on this also with
Melton who independently these guys
started independently working on a
similar thing and then ended up merging
their efforts uh so I'm really glad oh
now is the music or no okay um anyway uh
yeah so the uh they managed to create a
new portal Network client implemented in
Java so portal which becomes very
important piece of the protocol with
four Force coming uh coming soon uh yeah
it's uh it's really amazing to see that
um you've been able to to take such a
big afford and start a serious project
so Derek please uh go ahead yeah yes
thank you
um all right yes so I am Derek
unfortunately ly my partner melson
wasn't able to make it um but yeah our
our project our joint project is uh Basu
portal clients Samba so
yes so I guess I'll give a brief
introduction to portal um because I
think a lot of the reasons that I wanted
to work on this project were just
because I found uh portal to be such an
interesting topic and so there we go so
for the purposes of Som up to the scope
of EPF um which is uh just for a few uh
main roles through the history sub
network uh portal Network provides a
solution uh to the proposed changes in
EIP 44s that allows for the DraStic
limiting of Storage storage requirements
to run a node um as well it also
provides a way to handle the sharing of
data in a way that uh not only can
handle scaling but actually benefits
from scaling and these benefits also
directly link to ethereum usability and
the uh lowering the barrier of Entry to
participation uh in the ethereum
ecosystem and then as well outside of
the scope of EPF as we continue uh
portal also contains proposed solutions
to managing State and Beacon chain
related information although this is
somewhat more preliminary
oh there we go and so now moving into
the the role of Soma in Portal
um there we go uh so there's kind of two
main roles s can play in the in this in
this big kind of puzzle and so one of
the big roles is of course to fill the
Gap in the consensus hyperledger
ecosystem in terms of a long-term EIP 44
solution in regard to the existence of s
but this also works to increase the
portal client diversity um in addition
to ease of use in terms of the Basu
execution clients but I think the other
one and potentially more exciting in my
eyes is its unique position compared to
other portal clients and that um it has
a way to easily bring portal
functionality to different uh Android to
the Android ecosystem with the benefit
of the inherent modularity of java but
also its native support and so it's a
very realistic goal to expect that
sample could simp simply be built on top
of as a library for other mobile or
resource constrainted applications
and so moving into where Samba is at
right now um my my initial proposal at
the beginning was extremely ambitious
and I admit that obviously not quite
where we got to but essentially this
proposal is a model of where will
be at one point and so all
Communications in related infrastructure
uh including um dis B5 portal wire and
UTP are eventually going to be
implemented uh mainly UTP at this point
is the one that we're working on um
and then to have a fully functioning
history Network as well as full function
of State Beacon State and Beacon to the
point of present portal development as
well as full interoperability with other
Port portal clients and
so uh our updated goal range of course
for the the scope of EPF uh at up to
this point uh just pertains to what we
figured would be reasonable in this time
scope but essentially that's to get to
have had a primary communication
infrastructure including a Discovery V5
and then the portal wire um un UTP was
some a big setback in the sense that um
there is no generalized UTP library and
so unfortunately we're to piggy back off
of anything like that and will have to
be something that we sit down and do
ourselves um and then uh to have
infrastructure in place where his
history Network function and so then
moving on once we have that
infrastructure and that we have is to
have generalized internal testing uh we
wanted to have SSD sterilization D
sterilization exposed API infrastructure
metrics and the only thing on the list T
that we weren't quite able to achieve in
this goal range is Portal Hive testing
unfortunately just given the scope is
we're very close to having had it a lot
of it was just have putting little
pieces together to to test it but we
figured it'd be better uh to just add
more functionality before we go and add
some just small pieces of testing as
opposed to now we can as we go forward
add a lot more uh tests and then um
prove that we can then interact with
other clients
and so this is a general diagram of kind
of how Samba functions and the the
layout of Soma and the big two pieces on
the bottom here are the ones that really
took the the the brunt of the time is
getting all the the uh base protocol um
communication systems in place and so
that includes like inbound outbound
communication um whether that be coming
from a client there's a different way to
handle requests coming in as opposed to
sending out requests or then receiving
inbound requests or responses and vice
versa um but then of course there's
other uh pieces of the puzzle such as
the API which Melton had done and Expos
an API so we're very well set up to then
start actually serving different pieces
such as the dis5 and history portal
history uh apis um and
so moving forward in the uh in the
project uh the future Soma obviously the
development of somba does not stop with
EPF uh the project is in a very good
place infrastructure wise and has
achieved a lot of Milestones um in terms
of functionality like at this point
currently we are able to send and
receive all different types all the
available uh portal wire packets uh to
all the other portal clients um and in
terms of certain packet types such as
ping pong obviously work well and then
uh we are able to now up to this point
uh almost full functionality on just uh
sending and receiving nodes in terms of
building our um
uh are are U peers and
so um moving forward then the continued
development will eventually bring us up
to date with the other portal clients
and so that that is to me also an
exciting uh part of being a part of
portal is that um for for the
development of Samba we have uh EPF but
then also there after moving after EPF
there's a portion of getting fully
caught up to other portal clients um but
then in in terms of I guess my
experience it's also very exciting in
that once we get to that point it's a
entirely different development Paradigm
where instead of catching up and
following different specs it's now
actively participating in development um
and so th this development is much more
something that I will have to learn as
well and that kind of brings me to I
would like to talk a bit about my EPF
experience because I think that was a
really big part of uh this program for
me is just a lot of the learning that
I've done and so I think for me there
there's definitely too many skills to
list here um but if I were to sum it up
EPF was my introduction to ethereum
before EPF I didn't know what ethereum
was I didn't even know what blockchain
technology was so for me that was a huge
just to get into the space and learn
everything um and of course that brought
a huge confidence boost in terms of
developments I previously you know I've
never worked on such a large scale
project before and so those
organizational skills are required to
create that project was like huge to me
and definitely gives me a lot more
confidence moving forward in in terms of
developing projects and continuing to
work on this project um as well I think
I acquired a lot of new learning skills
I for me it's I think I could best
describe it as I learned how to learn um
so I guess the biggest example for in my
case is going through and learning the
consensus Discovery V5 Library I had
never had such a large scale library
that I had had to learn in the past and
so uh learning this library and then
automatically going and going over to
the teu library and then learning uh the
SS internal infrastructure was a way to
then quickly uh apply the skills that I
just learned to then learn a new library
which was a really great experience um
and then as well it gave me a project I
really enjoy working on and of course
will continue working on um and so yeah
uh the SoMo repository can be found uh
on 's G GitHub um I hope that one day we
can have a hyperledger or consensus
repository uh for that name um and then
of course I'd like to give a big thank
you to Melton unfortunately wasn't able
to be here but it's been a great a great
partner great working with you Melton if
you're watching um and then uh my friend
and Mentor Colby uh so unfortunately I
can't say that to Colby if uh if you're
interested more in Portal um his talk is
going on I believe on stage two right
now um and then of course EPF organizer
watch and Mario it's been a really great
experience I really thank you for this
but yeah thank you thank you so much
Derek I yeah I really appreciate it
really appreciate your words and also
your experience with EPF that's amazing
to see and uh yeah yeah I'm also going
make it I met him uh in Argentina like a
week ago and like he's he's also a great
person couldn't couldn't come but uh he
will be still also contributing to
sambba um anyone anyone uh has a
question about portal about uh um the
best java implementation yeah
responses are functioning um and then as
well exposing apis a lot of the stuff
also just includes serving like
information that comes just from the
Discovery V5 library and so that's kind
of the part that we are now stepping
into is getting all that set
cool thank you so much uh we have a
space for another question if there is
uh anyone any comments any questions
yeah I mean it's it's very important
topic in the L1 clients we need
diversity in the portal clients so yeah
I really respect the work man and if
there
are's just make sure to you know hold it
straight so I can hear you because too
far uh yeah I'm just want to you need to
plug it to your computer so because it's
your from your computer so it needs to
be plugged in
um oh I don't know about maybe just turn
this off and it should work now
just do you know how to step clicker on
MacBook clicker on MacBook
man it asks for some there was ask to
for there was uh there was there was a
prompt no you plug it in and there was a
prompt that you need to set it
yeah
okay doesn't doesn't work uh can you
open the there was another window which
ask you for the prompt you minimized it
on this screen on the screen with
presentation
there this screen in the middle there
was can you see all the all the uh
Windows
please there was a window which asked
you for like uh setting up a clicker or
you can just you can just use use use
the computer if we don't have a clicker
it's okay we should
start yeah just use the yeah that works
so just okay uh sorry just click her
drama again like technology right uh so
we have Adia next thank you so much for
being here and my honor to welcome Adia
here because uh he've been uh uh working
on very important project as well on
verly implementation in red which is
like incredible piece of work as well
like he taken this upon himself to start
working on the ve Vector commitment
Merle trees in red client uh so now red
is very open to contributors it that
some such an important feature is done
by some random contributor basically
right so uh first they were like hes to
uh to look into it and like wanted to
rebuild it themselves but then when they
actually saw the code they accepted it
and gave you gave gave feedback and
started work with ARA so it was uh it
was a bit of a drama there to actually
get it merged but um I think it's
amazing that we managed to uh make such
a significant contribution to one of the
the the execution clients so great job
ATA and I'm looking forward to
presentation yeah the stage is
yours
uh thank you every everyone uh hey so I
am Aditya I worked on the project of
veral tree integration the r uh
basically the uh code involved the
modification in the red to enable the
stateless
mode so before moving forward I would
like to discuss uh few points about The
Verge and the work trees so the main
goal of the Verge was earlier to enable
the stateless client uh in uh etherum
blockchain but now the Verge represents
a very ambitious and a bigger goal to to
verify the whole consensus and execution
using
snars but currently the stateless team
is focusing on enabling the workes in
the execution clients so my project
involved only uh implementing the
stateless mode using the wal trees so
the stateless mode first can also be
enabled using the Merkel Patricia trees
but the witness size are very big using
the mpts uh in average case it's about 3
KB and in worst case it might rise to
about three times faster so so the
concept of work trees was introduced uh
the main feature and the main advantage
of the wal tree was to reduce the size
of the
witness and it will be used as a major
State commitment tree in the stateless
mode whenever The Verge is
activated so yeah so uh now moving
forward uh what were the changes I
implemented in R so actually the changes
can be divided into two parts uh because
the devet 7 which is which was launched
about a week ago uh it is a definite use
for the work testing by all the clients
such as the nine get R and every other
execution layer clients so this devet
targeted actually two VIPs the first one
was uh 4762 it was related to gas C gas
cost changes and uh the second one was
structure the so the first modification
were related to the EIP 6800 which were
involving the tri structure
modifications so the main change here
was that in the mpts we basically
maintain two structure one is for
account tree and the second is for
storage tree but in the veral world uh
there will only be a single tree
structure it will be uh handling the
commitment of all the storage uh of of
the storage as well as the account data
which involves the code example if you
want to access a account data you will
be using zero zero value which is
suffixed to the original key and
included for the final key value and
similarly for code value and the storage
value so now instead of using the two
separate version of of tree we will be
using a SE single veral tree for
managing the
state now what were the changes actually
needed to implement this first of all uh
the r uh the two the St actually uses
after every transaction in the block
inter intermediate State Route was
computed and after block finalization
when the final State rot after the block
was computed
were replaced by voal rout provider and
the voal route provider were now
handling the commitment process in the
r now there are some uh Improvement in
the uh efficiency and the processing of
the clients for example currently the
mpts uh implemented by R uses C caching
mechanism for storing the uh
intermediate Roots they use incremental
update so that they do not have to
compute the whole route again and they
can just partially compute the roots and
the third one was paralyzation uh so re
has enabled account paraly the model is
currently working fine uh one of the
major changes that was introduced were
in the database structure uh for example
earlier there were two tables maintained
one was for the storage and one was for
the account but now uh there is only one
worky structure which needs to be
maintained which is responsible for
handling or the state
commitments so these changes were
related to EIP 6800 basically the root
uh the try structures and the commitment
structure in the clients so next major
EIP which was targeted by the uh devet 7
was 4762 uh 4762 is basically the uh
modification n to the evm
for uh when the witness is constructed
all the gas cost changes were needed to
be implemented in evm because now
construction of winess were included in
the block production and all these
accesses will require extra extra cost
for accessing the various data required
for witness
construction so basically the EIP
targeted two types one was one was for
access event and second was for right
events access we basically reading the
data for constructing the witness and
right was for writing the data while
constructing the witness right so you
can see all the opcodes that whose uh
gas cost will be modified when the CIP
will go live all these opcodes uh where
need to be uh the gas cost model for
this uput were
modified yeah and this VIP also designs
how uh how we are going to access the
pre- compiles the system contracts and
uh the gas cost of various outputs so
how the how do how I implemented in it
revm so I referenced the implementation
of get and nethermind for this purpose
uh these are all the functions that were
called whenever a particular op code is
executed uh during a transaction uh this
functions will be called by this op
codes uh they will be adding the cost
for witness access during the
execution the main logic was the access
key function which is internally called
by all of this function exess key
function is basically responsible for uh
basically charging all the gas cost it
it checks whether it's for exess it's
whether it's for right it maintains a
list of all the exess sub trees and
modified trees it charges accordingly if
the tree is not yet touched it charges
the right Le cost if it's already uh
touched then it will be charging the
right
cost so all these functions were
separately implemented and they were
inserted wherever the opcodes for that
particular function
called now the most important part of
the project were the test vectors so
test Vector basically the execution spec
test which is developed by the stateless
team right now they are they are the
major like the major uh the project even
though
all the changes were implemented but
this test vectors they cently consist of
more than 100 test about I guess 200
maybe so this taste Vector actually not
only judges the code but also they are
responsible for handling the various
minute cases and the edge cases of the
clients so after I was able to pass
about 60% of this test Vector but there
were some ede cases for example uh you
are uh for example you don't have enough
gas for adding the data to the witness
but still you are adding it because uh
they were supposed to be added during
the propagation so that AG cases need to
be encountered here so some of the test
are still yet failing but I will be uh
altering the code
accordingly so 4762 and 68 there is a
third EIP as well 7709 uh this EIP I yet
to touch it is basically handling the
block as
instruction the plan
so basically I was able to implement all
the EIP and about 60% of the test
directors be fast but the goal I set was
very ambitious uh the uh basically this
work is going on for almost uh 1.5 to
two years so implementing all this thing
in three to four months were like a
hectic task but uh I like really proud
of the work which I have done till
now so basic modification for
commitments are done for rvm changes are
done and the try changes are also done
the the next phase will be for exam uh
uh what I want to say is uh like the
main things such as the construction of
witness propagation of the witness are
completed but the main feature of the
stateless execution is the stateless
mode where the block will be executed
using the witness which is provided
using the uh uh along with the block and
not with the existing state with the
local
client so stateless execution uses
witness is the next main feature this
feature is not yet implemented by G
because it's not targeted for this de
but it is live in the nether mind so I
will be refreshing the nether mind's
implementation uh for this stateless
mode now more into the future uh the
veral not only involves the try design
and the gas cost changes for storage but
the main problem the veral world will be
facing with the syn design uh how we
will be moving from the Merk Patricia
trees to the veral trees how the data
will be migrated there are some existing
strategies for this for example the over
root overlay root strategy which is
currently accepted and there are some
sying strategies as well for example
it's implemented in either nether mind
but the research is yet to be done for
this purposes so syncing would be the
next phase for the wal research and the
main target for the stateless
team apart from that uh the recent Z
technology have evolved lot there are
some improvements in the proving
technology so even though there are many
advantage of the work for example the
witness is updatable in the vle the
proof size are very small and the vle
code is almost ready for the production
uh we can shift to statelessness
directly but there are some research
going on uh to directly Shi to the full
whole verification of the consensus and
execution layer using ZK
knocks uh for that the wal is very uh
not easily used we are not able to prove
the voal TR structure easily using snar
so there are some proposed structur such
as stocked binary hash trees and circle
stocks us and proving uh hashes using
circus such as posidon and everything so
uh the main task will be of the
stateless team will be to go through a
state uh Tri agnostic design so whatever
will be the TR design accepted finally
the main changes related to gas cost
sying and witness construction would
should remain
same so this will this will uh be my
major tax I will be running uh syncing
with the stateless team and uh carry on
the work for the bre on behalf of
them final note uh EPF was a was a very
great experience for me I would like to
thank Josh and Mario for giving me this
opportunity uh this was the first major
project which I took apart from my
personal contribution so this provided a
very good direction for my future
developments I would also like to thank
Kev hre already implemented the rust
work TR structure uh which I used for
modifications in the R I would like to
thank ignasio from the team and Tanish
from the Nan team as well and Oliver
from the red team for mentoring me
through this project so like EPF is one
of the best Gateway for etherum C
protocol development and if you are
getting a chance you should go for it
yeah so thank you everyone this was my
project awesome thank you so much for
your kind words thank you so much they
appreciate it and we have a space for a
question if there is maybe like one uh
two questions anyone any comments yeah
oh there is one online uh are you also
going to impl the state three migration
approaches like the overlay three yeah
so uh the migration strategy is not yet
finalized uh overlay tree the design of
the overlay tree is not much clear right
now like it not at all implemented in
any client because research is also not
going into that uh because uh like
because we the the main target to first
get over with the gas cost changes
because we are the team is planning to
make life the gas cost changes before
the vge because then the other l2s and
should Geto accustomed to the various
gas cost changes but the transition and
the sink is not yet designed completely
and it's very vague right now so uh I
will be running inop with the stateless
team and whenever it will be ready I
will be more than happy to implement it
yeah awesome thank you so much um yeah
thank you so much Aria amazing yeah very
well yeah yeah the migration is still
kind of out in there so uh it's GNA be
future for we have V next uh if you're
ready um uh do we have HDMI you plug in
uh hi hi hi I have a question H for for
ARA yeah okay yeah so uh I think R uh
has a similar database design as arigon
which is path-based
so like while you were work implementing
like the veral try in R did you have to
like modify or like optimize in any way
yeah so uh the same data base structure
is in the Aron 2 not in the Aron 3 uh
and the this thing is all everything not
like mostly nothing is implemented in
Aron 2 and so from the Aron 3 team is
using arigon 3 for this and Aron 3 does
not share a database structure with the
re so with the path Bas structure I was
not I did not modify anything to the
past based structure I was I just
introduced a different table for it for
storing the work tree for the state
commitments so yeah like uh did you run
any numbers yeah so uh I haven't tested
my database right now but uh but I will
be getting the pre-images from the Aron
because they are able to Supply right
right now and I will be benchmarking the
performance using the pages and the main
net blocks
okay thank you so much uh yeah thank you
so much for extra question uh we get a
switch to uh richer and I'm sry click
doesn't work
so use your keyboard and um uh and uh uh
here's your mic and please just hold it
close yeah yeah thank you so much okay
our next speaker uh rich and again it's
my honor to welcome another very
successful very talented fellow on the
stage here today and Risha because uh
she didn't work on only a single project
she took upon a lot of work work during
this fellowship and is just her first
presentation today and um uh it's
um uh do we have uh yeah the
HDMI yeah yeah the video very well yeah
so uh uh Rich also contributed to the
ver fors uh by uh you creating SE
bindings for the constant in
cryptographic library and uh yeah uh the
stage is yours please go ahead
it's
uh is it is it your it's your laptop but
you need to change you need to change I
don't know how you do
Ms do you notice how on like developer
and Technical conferences all the tech
just fails it's like the hacker
conferences
right yeah yeah we just need to change
the the display uh yeah
sharing uh so hi everyone this is my
project for C implementing C bindings
for work IPA so yeah I was mentored by
uh MIM from Constantine and Agnes from
nimas Team for the project so yeah
talking about the project how it all
started like uh um I started on the
project pretty late last month so uh
basically uh the idea behind the project
was the Constantine Library provides a
lot of cryptography uh Primitives for
ethereum and non- ethereum uh backends
so uh yeah so the project basically was
developed since there was provided by
the uh Constantine and the ners
implementation of the work drive was
significantly good
so yeah so that's why uh the the way to
use the constant uh constanty library
for integrating with other clients was
to export the veral IPA implementation
using the C bindings and this is what
the project was about so how it all
started was so the first step was
implementing the bindings for the uh
Edwards curves and the Vander
vanderwagen curves along with their
operations
so yeah this basically involved um uh
since uh the Constantine Library already
had implementation of Wonder vanderwagen
and the Twisted Edward curvs but they
were not uh exported through the sea
bindings they have sea bindings for some
of the curves like the elliptic curves
or other palaz and Vestas curve but the
adver curve and the vanderwagen curves
were missing so the first part of the
project was implementing the SE bindings
for these curves and then since the
Constantine uh code base is basically
generic like it is not protocol is
specific to ethereum so the next step
was to implement the uh ethereum
specific wrapper for the veral IPA
implementation and then exporting that
to be available to other clients and
next was to integrate it with the client
uh teams uh so presently I was involved
in working alongside with the B Basu
team for integrating the Constantine
bindings uh in their Bas native so yeah
so how it went so I started with
implementing the bindings for Twisted
Edward curves so they this involved all
the operations they had like the
addition
addition multiplication scalar
multiplications bad of find and so the
mainly the curves which were used for
the worker here were aine and projective
and all the operations which
were uh yeah and the operations for
these were basically needed to be
exported so the bindings were generated
for the fine and projective curves and
then implementation of the vanderen
curve was also done in a similar way
where they had the uh scaler field and
uh scalar field implementation scaler
field specific operations uh exported
through the sea bindings so this first
step was completed and was approved to
be
merged and here's the pr for it but
there were some uh issues since um
Constantine is still not armed
compatible so there were some issues
which uh
I was facing while working on like
proceeding for further with it as there
were some some uh GMP headers which were
not being able to uh the Mac was not
being able to uh find it so like had to
work on it with the Linux
systems uh so the next phase was to
implement the um generate to make the
ethereum a specific bindings for the
Constantine code for veral IPA so uh
yeah so this involved basically
implementing the uh functions which were
needed for
uh to be integrated in the client
architecture so it was the proof multi
proof commit and verify and multi verify
so uh and exporting these through the
export C pragma so this PR is uh so like
this work is also done and it is open
for review talking about how the next
steps will go on so yeah uh there are a
lot of problems which are arising due to
the arm incompatibility with the the
Constantine so first like we need to
figure out if there is some weight so
that we can proceed on with the testings
and then test the SE bindings with the
uh test the SE bindings for the curves
and the veral IPA from the Constantine
and then finally integrating it with the
client teams so uh we have a Basu team
with which uh the work is currently
going so there is a PR on which uh uh
the work has started but since the
bindings were missing so we need to
implement integrate it with
that and yeah so this was all
about thank you very much rich thank you
so much yeah uh any questions any
comments for Richa we still have we have
ton of time to
uh
yeah um what what uh problems are you
are facing we facing with uh the arm
specific is it is it uh uh code or why
why specifically there's an arm problem
oh so there were some uh uh so there
were some uh there's a library GMP which
is uh which is available in the sea so
that is that requires that x86
architecture so the Linker when I am I'm
using it in the arm architecture the
Linker is not able to find uh the path
so I tried several ways like including
the exact path for the GMP because GMP
is in the system so I tried including
the exact path and uh several other ways
like uh
manually trying the Linker to link to
that GMP but that didn't work yet so I
have to find other ways to see it nice
work thank you awesome uh any more
questions we still have a space if there
is any question comment for
richer yeah
yeah this one test yeah I was just
wondering uh can you say like what went
really well with this project um um just
in general sorry uh uh what went really
well so uh what went really well was uh
like basically I got to know about the
Constantine uh Library like for on my
part like for the learning was the U
mainly thing which I actually enjoyed on
and apart from that uh while working on
the bindings uh like the uh veral try
was already implemented so exporting
those bindings were quite easier because
the bindings like generating bindings
there was already a template which could
could be used directly to uh have the
data types or the curves or the
vanderwagen or the associated parts to
be integrated and there were several
segments of the code which needed to be
modified because the library has a lot
of links internally so that was also
easier because of the way the code
is awesome thank you so much again uh
yeah good up for richer again thank you
very much um yeah that was that was
lovely uh yeah uh and again it's not the
the last time you see on the stage here
today so let me un unplug you yeah there
we go um next step is uh La yeah if
you're ready also plug it in
okay very well uh we have another uh
lady fellow on the stage today and uh
actually similar story to Rich because
La was also working on multiple projects
and to be frank La took a quite a big
bite of the Corporal contributions
because it was uh um a lot of work to do
and uh although she wasn't able to
finish all of it she still made a great
contribution and today she will talk uh
talk about uh the HTTP client
implementation in Lighthouse and her
contributions to it so yeah the stage is
yours thanks Mario and so I'm Leah
sofware specializ in rusts and you need
to switch your slides here
yeah okay no you need yeah
oh no you need
to I think I need
to um not this
oh okay I mean I can duplicate
otherwise
okay
amazing
um yeah thanks for being here so I'm a
software engineer specialized in R and
as mayos mentioned um this project was
in fact supposed to be a one week's
maximum good first issue to start it in
Lighthouse and not my initial project
but as we are we are all developer and I
think one of our defaults is like to
underestimate uh developer time
development time um so why um so I pick
this project of course to go deep into
uh Lighthouse and understand before uh
better before transitioning um um to a
more research project project uh but as
a developer was interest uh in big
refactor um so why uh do the team wants
to move um between warp and axum but
first why it is what is warp and what
are axum so there web server framework
creates in Rust and one has been
developed a bit long time ago so is a
bit low level um low level oriented and
a bit less easy to use despite axum has
been developed by the Tokyo team Tokyo
organizations and a bit easy to use and
why do they want to move um of course
they want to move uh for easier fuse and
because WP types which is very focused
on filter uh filter cies is a bit
difficult to handle and also it's very
um something you need to understand is
like only one part is still using Waring
W in
Lighthouse and um all the other part are
using axum so the idea of course is to
unify but um one also important things
uh in Lighthouse right now they want to
have this modular approach so they will
size the opportunity um with the
refactor to have a more for all the
different Bon IPI end point um to
refactor the function andler and then it
will be easy to integrate in a new
repository called ether pii where you
can have a special tra biton API and
just referencing to this Lighthouse new
end point and
L um just to mention this is the most uh
sexy slide I've been doing in my
consultant career before uh just to
mention as I've been saying only the Bon
node API is
targeting uh is targeting in this
project um previous one previous one
have other part are already uh AUM
oriented um so refactor
challenge um
first intuition is like to remove
completely the previous crate our war
just to replace by axum and to find
equivalent but it was definitely not a
good idea because we wanted a
performance test before um before and to
have a more soft so approach and just
say oh okay I want to use this new crate
uh let's replace and not have any Matrix
about
it um so what where so very um very
simple condition first to to test the
different uh to test the different
implementation something I didn't
mention that the work was already
initialized uh by the lighthouse core
developer and so we could already test
very simple endpoint
that has been refactor in um in
Lighthouse so we could do just uh
performance testing and have um tests by
targeting with a certain number of
requests and check and compare the
difference performance time between axom
and warp end
point so the result uh that is W is uh
faster than ax
and um probably because if you have most
of the time if you have a also a crate
that is easy to use it means it can be
more interfac and so it's most of the
time um a bit slower also something just
by curiosity I was looking at
is it was that if the Tas spammer has an
incidence of course it has one but the
difference is very very important so our
idea right now is to refactor um to have
a better under on the Tas spammer um
yeah we there are still a bit work to do
to handle a bit in a bit better way the
Tas spammer in the AUM
refactor so still still a lot of step to
do as I mentioned I'm already working on
it there are this optimization of the T
pammer and I want uh especially from
tomorrow uh have this great occasion to
meet the lighthouse team um to define a
very uh specific plan in the GitHub
issue and have hold this transition
issue um and any public um
also any developer can take part of this
um transition the more it is split the
more it is better for anyone to take in
and yeah just to deliver the task
after and as I mentioned if we are
refactoring all this endpoint Handler
after they can be referenced uh in this
ATM IPI no
repository um in the specific uh beon
API tra
yet of course um EPF is not just about
uh technical or at real knowledge of
course it's maybe 90% what is targeting
but it's a lot of personal Insight um as
a developer I could have a better Vision
how to have more manageable piece and
yeah do what I call Baby stape um when
you are doing a big refactor especially
with Lighthouse r
a very huge res
repository um work is not finished and
there are maybe 80% still to need to be
done um but I still have a better
overview about a lighthouse repository
especially is Bon
IPI and yes M reflect as a
developer um yeah thanks a lot to thef
team uh for hold the help guidance and
to the lighthouse also developer team
awesome thank you very much LA and yeah
as I mentioned um so there is all this
timeline and as it is finished I can
really start the fil
implementation thank you yeah awesome
thank you so much La yeah I appreciate
it um yeah again it's amazing to see
that you still pulled through and
managed to uh make it till today folks
we have we have questions for yeah um
yeah we have ton of time so feel free to
ask uh did you consider any other web
framework other than x some like rocket
or anything
just so following that question um where
did did Tower feature into the
conversations about this transition talk
um it was proposed as a
project just interested in whether that
was you know going from what to a I
don't know I was just thinking about the
components
think wanted some just to use axum
because it was already used okay you
back but I think they can
answer also yeah if there is any
lighthouse inside as well feel free to
add a
comment yeah thank you so much but yeah
if there are any more questions or
comments folks um yeah yeah otherwise
thank you so much again La yeah I
appreciate it thank you very much um
awesome awesome uh and next is Sayan I
believe so let's take a minute to plug
in again
have at all
just so you know this is all make books
having these issues not Linux
computers and uh yeah we're almost there
just yeah drag the presentation awesome
uh yeah I think there is some light mode
or something
yeah we just get a colors right and we
are almost there and let me meanwhile
introduce Sayan our another speaker
another successful fellow thank you so
much for being here yeah yeah I think
it's good we can be a bit orange yeah uh
yeah thanks so much for being here uh
Sayan another uh follow working on
Lighthouse another fellow contributing
to Lighthouse consensus client uh this
time uh uh integrating the logging
framework tracing into Lighthouse so
thank you so much for being here yeah go
ahead
hello
everyone uh so first of all let me say
this that uh my project Isn't that cool
uh it's kind of like um tedious and uh
large size chore uh like you guys will
uh get to witness many cool projects so
I'll will just keep my presentation
short and finish this uh quickly okay so
okay um I'm Shion as you can see already
um and I have been uh trying to
integrate tracing uh into Lighthouse so
first of all a little bit of background
for uh those who are not familiar with
the rusco system so uh tracing uh what's
tracing it's a cool uh acing friendly
logging framework and uh but why did we
want to migrate Lighthouse from SLO to
tracing uh because SLO is kind of lame
and we need to pass a logger everywhere
uh while using SLO slog is not Asing
friendly at all and it makes uh
exporting Lighthouse crates kind of
annoying and pretty much painful and uh
on the other hand we have uh tracing uh
uh which provides additional uh
contextual info that means uh we get a
better debugging experience we have a
modern to logging tools and it makes uh
easier for external code consumers to
export LH grates um and most of the
dependencies in the lighthouse already
uses tracing so we might as well uh do
the shift and it's yeah it's quite Asing
friendly as well yeah just just uh way
better than SLO in my opinion okay so
what have I done so far
um so uh there's the pr uh if you want
to go check it out you can just uh go to
the lighthouse repo
uh and in the pr section you can uh sort
it uh by the most commented PRS and it's
currently uh the most commented PR over
uh in the lighthouse I'm not sure if
it's a good thing or bad thing not not a
great thing I guess but yeah um so so
far I have migrated like the pr isn't
merged yet but I have migrated the whole
Lighthouse code based from SLO to
tracing uh and I have uh almost messed
with almost every file there was and I
uh reimplemented the custom uh formatter
uh and I reimplemented the S logging
component and also replaced the slogger
which was used for um the file logging
stuff and uh converted it to like
shifted it to uh rolling appender yeah
so let's move on to the next
stff um yeah so this is uh what the logs
look like right now uh and we Tred to
make sure the format is exactly almost
exactly the same as we had before so
yeah and next up uh let's uh discuss
some challenges
um uh okay so one of the challenges that
we faced was to uh keep the format same
uh because many uh external and internal
infra depends on that uh in U depends on
the Lo lock format to be the same I just
got to know that siren kind of depends
uh on the lock format uh so yeah uh we
just uh that thing is uh before adding a
custom tracing uh for uh formatting
layer and this one's what we have now
yeah so we just just just try to make
sure there isn't much
difference um yeah and other thing was
um tracing uh doesn't have a crit macro
inbuilt so we had to implement uh
implement it on our own so here's what
the crit looks
like um
and this was uh the trickiest part uh so
the previous implementation of s logging
was kind of complex at least for me uh
to understand uh and shout out to uh ede
for helping me understand that code so
basically what it was doing was uh they
were using slog as sync uh it collects
all the logs into a single task and then
the task could uh then print and write
uh two files as synchronously uh and the
HTTP API wasn't a separate so uh it
couldn't uh see any of the logs so they
had to modify the uh SL uh sling task
thing to create a channel and then uh
HTTP API could just get uh get this
Channel and uh then the slock Tas could
just uh dump all the logs into that
channel and for some reasons slog uh
Records didn't allow sending the data uh
through the channel so they had to make
the Asing record thing if you check it
out it's uh it's like uh complex I'll
say that because because I didn't
understand what was going on so no point
in making
comments okay so with uh tracing it was
super easy to implement um uh we just
had to uh set up a tracing subscriber
and uh it was almost done and this is
what uh the logging looks like right now
and uh we had to just remove the whole
loging part from before and
reimplemented uh reimplement on our
own okay and other thing was uh span
versus uh slogs o macro uh so both of
them are kind of used uh to get
additional contextual info although they
work uh kind of differently um and uh
thanks to atan for helping me out out
with this one because uh the first thing
I did was just I replaced all of the SLO
macros wherever they're uh being used to
span which uh like it was functional but
wasn't U uh working uh as it should have
uh so here's uh the P both of those PR
are from Ean um and he managed to add
the spans in the uh slasher service and
E RPC service so uh and I I'll also have
to add a lot of spans it's on the road
map next but yeah uh they are just add
so that this we can get the service uh
to show up in the logs which is
currently uh many of the service aren't
uh showing up on the logs so yeah some
uh just random challenges that we faced
are uh we tried to break it up in
multiple PRS but it it would have broken
the S logging so we had uh one single PR
and it has been quite a bit of back and
forth uh and uh last thing was sever uh
skill issues because I was really really
bad at trust and uh development in
general so yeah
uh so next steps um uh adding more spans
as I've said that a bunch of services
aren't showing up in the log so uh we'll
need to add a bunch of more uh spans and
there are still uh lot of feedback fix
iterations uh left to do and we'll need
to do more QA and testing to make sure
we don't break much stuff and other
thing was yeah we need to add uh some
Flags to enable or disable additional
context and overall all experience uh
was great uh the APF last 5 months were
uh like uh went uh pretty well uh
working with the lighthouse team was
great thanks uh to you guys for being
here and attending the session uh bit
better at trust I think it's debatable
so and loved the office hours uh got a
lot of Alpha from
there and finally uh huge thanks to the
lighthouse teams especially uh age itan
and mac and special thanks to atan and
Mac because they are dealing with that
mess of a PR like there all the comments
and all the back and forth thanks for
being so patient with me thanks to H for
helping me out with the S logging stuff
and uh thanks to Josh and Mario uh for
conducting the whole uh EPF and uh just
to just thanks guys you you guys are the
OG's so yeah that's all thanks for
sitting through my weirds Stu whatever
that
was yeah uh thank you very much San for
your kind words but please don't be so
humble I think you still did a great job
it was a learning experience so guys I
mean I hope that it was productive right
come on like he helped you right so
don't be so humble it was great yeah and
uh now we are running ahead of time we
have like ton of time for questions so
please like yeah give give him the
business give him some questions
comments anyone yeah
so are you using or tracing uh traces
between instances or separate client and
server are you setting remote parent IDs
for traces for analyzing them in for
example grafana Tempo or other not yet
not yet not yet okay I haven't set up
yet and uh what are you using for
analyzing
logs uh I'm not sure if I'm using
anything uh there's a called yer or
something something I am not using that
yet so there's a lot of testing and
keyway and analy only file appender yeah
file appender we have loging stuff
working uh normal loog logs are showing
up yeah got it uh so any plans for
adding metric layer like very cool
Library yeah there's uh so there's
already the metrix layer that uh was
already being used for the LI PTP and
disk V5 but it's not functional yet
because I messed up a lot of stuff so
once that's done uh I'll make it
functional it's if you see the code
there's the Matrix lay added in the
subscribers stuff but it's not
functional
yet did you see any uh memory
improvements by removing the it's like a
global
log sorry object did you see any memory
improvements between I hav you Haven
haven't
cheed um any comments or or questions
yeah how do you choose which memes to
put into your
presentation I don't know I uh most of
the memes uh like I added uh last night
so whatever it's OC memes it's you are
the you are the OC right OC orig Creator
yeah yeah yeah
yeah that's great um yeah we still have
time if any anything anybody yeah
yeah could you that was a great talk
could you could you share more about the
the um the fellowship application
process and how that how you came to
that opportunity oh okay so I guess
these two guys are the best guys to talk
about this but um I had some friends who
were fellows so I got to know about the
fellowship from them and uh I did some
contributions in the execution spec
tests and like uh some of the clients
and before that I was doing some
security research stuff I don't know if
that helped or not and uh once the
application opened up I did it and uh I
when the study group was starting out I
just uh like did a small typo fix I'm
not sure if that's the reason because
I yeah yeah that's why we hired you man
mean PR that no no yeah yeah just to
elaborate on it a bit because uh think
thank so much for for asking for that uh
we are uh going to do the next cohort
next year uh now we have kind of a uh uh
the regular schedule where we organize
these cohorts June to November so it's
five months starting in June with
applications opening in May uh but one
important thing to mention here that
this program is fully open and fully
permissionless uh it's like I believe
that it became kind of the place to come
to start contributing to core etherium
but still it's more of a like word of
mouth and uh general knowledge so like
please spread a word and uh uh invite
other people who uh who might be
interested um we are also doing the
study group which is more like um uh
just St learning process uh it's a
there's a Wiki that we collaborate to
create uh EPF Wiki all information is
there about the study group and so on uh
next year we're going to be iterating on
it improving it adding more materials
and it's something for people to get
ready for the fellowship or just like
those who are not ready yet who don't
have the dev background ethereum
background they can take uh like month
or two uh of the study group with it for
two months uh they can they can study
those parts they are interested in and
then it's much easier to start uh with
the fellowship yeah uh and again like
it's fully open so even those people who
are not uh submitting applications or
don't get a stien can still still work
so bunch of the people that you are
going to hear today were not paid for
this we're just dedicated to ethereum
we're so excited to to work on this we
get them the trip to Bangkok to to award
them but still like it's just pure
dedication of many people that's what I
love about this community yeah and again
thank you so much Sayan ah yeah great
talk uh yeah please don't be so humble
it was great yeah and yeah give it give
it up one more time yeah thank you so
much
um and uh next up is I think Gia again
maybe no no Boston sorry yeah come come
guys yeah we have uh yeah we have a few
minutes to set up so got to have uh
another 2 minutes break we have these
session back to back but uh yeah let's
set it up
like
this e
this doesn't work
to
my
okay everyone please uh take a seat as
we are starting with another uh another
around of presentations so my honor to
welcome uh uh more speakers on the stage
here uh Bastin andrup Pam uh with uh uh
uh another another presentation I want
to I want to highlight that uh butin
here uh these folks have been working
perly as I mentioned before and um it's
again uh huge thanks for work for your
contributions making it all the way here
and their work is uh again important
contribution the light client server in
prism so um yeah important part of the
stack for people who want to run light
white white glans please just yours go
ahead guys
yeah so hey everyone I'm Bon this is
yeah hey everyone I'm rupam and we have
been working on uh integrating light CL
server size support in prism uh and for
the project intro like light nodes are
basically uh nodes which you can run
which you use significantly less
resources than a full node but there are
some drawbacks uh how a light CLI works
is basically the maintain this chain of
block headers until the latest block
header yeah uh we get this trusted block
route which is available in the network
and using that we can fetch a range of
updates after that we get the root hash
which is which can basically be used to
determine the uh latest block header so
uh there's this reference of the like
clients after the merge by itan it was
covered in Defcon 6 I guess yeah so uh
you can check it out it's really nice
and yeah yeah so um if we're looking at
what a light client update looks like
it's uh simple structure it contains the
blog header so the light client can uh
basically save that blog header it has
the current sync committee public Keys
uh the syn committee is the
block header and uh the light clients
basically verify that blog Header by
checking the sync committee signature
and there's also something else called
the uh sync committee uh bit which shows
which of these uh 512 uh validators
actually did sign the block header and
also there is this uh next sync
committee which is the sync committee
the 512 Sy committee for the next period
so you can basically check when you get
the next block header that the
signature uh checks out and this is
actually part of the chain uh yeah so uh
how the network works right now is that
uh we have this uh Beacon Network which
Full nodes uh talk to each other and
connect to each other and we have a
light node on the side which basically
asks one of the full nodes for these
updates over and over again to be able
to uh stay synced and then we have this
RPC provider that uh we might not trust
so what a light not what a light client
actually does is that it uh gets that RP
gets the data from that RPC but uh
instead of just trusting it it checks it
against the state route that it gets
from the full
notes yeah yeah and what we did in this
project was basically implementing that
part
which is how the light node talks to a
full node and gets those
updates uh yeah and there are four
endpoints the first one uh the bootstrap
using the block rout you have the
trusted block rout in the network and
you can just call the bootstrap on that
block rout and uh after that next yeah
you'll get you get a range of three up
dates let's say three here and when the
uh lightland has synced up to the latest
block header uh you can just keep
calling optimistic update or finality
update according to it
needs yeah so uh the project faces that
we had was uh we got familiar with prism
and how light clients work and then uh
we started implementing the specs and
then going back and forth uh with
testing and
debugging this is nice but this is a lie
this is how it actually is so everything
was uh
weird but uh yeah so the current state
of the project is that we implemented uh
the four RPC
endpoints and we uh implemented that for
uh like Forks after alter which is all
the forks that support light clients uh
we implemented uh two databases one for
light client updates and one for light
client bootstraps to store them and not
have to manually like comp compute them
each time uh yeah and also we
implemented saving updates and bootstrap
when receiving new blocks unfortunately
not
still uh hav we haven't still
implemented uh saving them while syncing
and also the P2P events and topics were
implemented so it's so these uh
functions are not only exposed through
the beacon RPC they are exposed through
the P2P uh Network work as well and uh
these are all visible on the uh EP of
flight client branch on
prism demo oh oh yeah we have a uh well
kind of demo so uh these are old but
this is the Nimbus uh server and this is
our server uh if we reload them at the
same time if we are lucky we get the
same result yeah kind of the same result
and uh so the difference as you can see
is that we have this bike which uh gives
us three more execution branches and we
have to debug and see why uh yeah but uh
mainly we can see that this is just one
of the uh RPC end points but uh this is
for the demo uh they're mainly uh
working correctly
okay uh yeah as of the next steps uh we
do have some work leftt and the first of
all would be we would we would need to
test it out on a test net and of course
like measure the test performance and
optimize it according to that we as also
Baston said before like uh we still
don't save updates or boot while syncing
which would be nice to have so we do
plan to add that too and after that yeah
uh adding light line stuff to e2a tests
and kosis yeah that's for mainly for all
testing for all the forks uh so that's
for the next steps and yeah almost all
consensus layers support l l now
including Nimbus low star so yeah prism
is also into the game now and special
thanks to R and Jo and Mario for the
cohort uh and radic for the awesome
mentoring here thank you thank you very
much thank you very much guys yeah uh I
really appreciate you sharing all of
this uh do we have any question for
Baston rupam yeah uh there
oh excuse me I have to scream um do you
know if there's any plans to add end
points to generate proofs uh sorry add
what um to add proofs so like the light
line date I said is very limited um
there's proofs to like prove the
execution branch and the finalize header
but say you wanted to prove something
else in the beacon State like block
routs um what would be super useful if
um there was an end point to say I want
approve block Roots at this index and
just return the hash I feel like that's
super missing from the LI line spec yeah
um um so the there are multiple eips uh
open right now that work on adding uh
getting proofs from the execution
clients not the consensus clients but uh
yeah there are multiple uh open PRS I
think at least four that uh point to
different parts of
uh the block where you can get uh
different proofs and then uh validate
them using uh all these roots that are
in the block header okay so but no
Beacon State proofs yet Beacon State
proofs we will we have the state root
hash in the block header so if you have
the proofs and if you have the data you
can check that against the state rout
yeah but so what I mean is like say you
want to prove anything in the beacon
State at the moment you need to download
the whole Beacon State SZ right and then
you have to like create the tree and
then generate the proof hashes yourself
but like for instance load star has a
endpoint where you can say I want to
proove this piece of the beacon State
and then they just return the proof that
would be really cool if all the clients
can add it because I think um sometimes
the apps using the light line data can't
get to all the data that you'd actually
need um for instance so um I work on a
bridge project um and we need to prove
if you have a finalized header that
another header is an ancestor of that
block and you need the block Roots
proofs to do that but it's missing from
the light line data yeah that is true I
don't think that's and there is such a
thing in prism right now but uh maybe hi
I'm from Prism so yeah the recently
actually that we had another person
asking us the same question so we are
actively looking into that so even if it
this is not in a uh official light
client um like future you know plans for
official specification it's definitely
an end point I would like to add so we
can talk to neighbors how they do that
and uh yeah I think in in the next few
months we we should I hope we will have
something to share with uh with
everyone awesome that's great news
thanks thank you so much R yeah and yeah
I just wanted to say thank you for for
this project I had lost all hope of
prism ever supporting the like client so
thank you yeah awesome
yeah okay uh quite quick question maybe
my understanding in proofs and stuff
quite limited but can you for example
bootstrap your light note from consensus
client and then start to query uh the
RPC uh provider for the latest data so
you you would unload basically all this
RPC cloes for RPC calls for from uh
consensus
clients so the yeah so you could use uh
if I'm understanding your question
correctly you're asking if you can uh
instead of uh asking the RPC providers
for data ask uh consensus note for
data okay the other way around the S
suit you get the latest blog data like
fin
latest BL from the consensus node yes
but can you do this from RPC client well
uh yes I think so you can do that but
like the point is that uh you basically
check and verify all these updates uh
one by one using the sync committee
signature so it doesn't really matter
where you get those updates from as long
as they like you have everyone in order
like uh from where you start and uh if
this is all dependent on if your uh
first trusted block rout is actually
part of the chain so if you've been
tricked to use something that's not on
the chain and is like on a fake chain
then you will keep up on that and you
won't know what's wrong but yeah you can
definitely get the updates from uh
anywhere and just verify them one by
one okay awesome thank you so much guys
uh maybe one more question or comment or
anything otherwise uh yeah huge thanks
again uh thank you so much for all your
work all your contributions yeah and the
presentation thanks thank you very much
uh awesome and uh next up is uh Rose so
let's let's set up another another PC
hopefully it will work yeah um
um okay starting in a minute another
presentation so this time uh it's uh
Rose uh speaking about her work on C
implementation of peer-to-peer this is a
new library for the peer-to-peer
implementation as you know um there is a
a big problem happening recently because
protocol Labs stopped funding the LI
peer-to-peer uh implementations uh which
means that now we kind of need to manage
it more independently so there is one uh
in work in C there is another uh project
in Fellowship which is uh uh
implementation in go for prism and we
will actually attach it to uh to this
session but we are starting with rose
here about the C so please go
ahead hi guys so my project is about as
visible in the screen so before diving
deep into my
do you have a signal no
signal have
signal or can you uh is it the same
yeah so can you maybe slides from so
instead of your computer they are going
to use those because they have those
slides
nice yeah so before diving deep into my
contributions I would
first before uh diving deep into my
contribution I would first uh uh like to
go through the introduction of the lip
P2P uh
so uh lip p2b is a modular networking
framework Library uh used by the to
communicate in a distributed network uh
it comprises of various protocols uh
like the transport protocols stream
multiplexers en transvers transversal
Etc and um uh when compared with the dev
P2P it's more modular and interoperable
defb2 is more concerned to the
ethereum's needs specifically uh so it
comprises of various components um so
the first component is transport so like
when a peer is connected to the internet
um it basically you know shares and
sends the bytes and uh bits to the uh
via IP and TCP connection which is like
the combination of the Internet Protocol
uh which is responsible for the
addressing and the sending data packets
and for the TCP transport control
protocol it's responsible for the
sequencing of the data like the the
order of the data package that have been
moved the vender it's been the same
order as of the uh as of the uh
receiver uh moving on to the next the
handick so basically when we have a TCP
or IP connection it's not secure we need
to secure the connection to you know
transmit the data so um so the handshake
is for the secure Communication channel
for the message exchanges so it compris
of the TLs protocol and the noise
protocol um also like the q i is like uh
one kind of Transport which has the
inbuilt encryption towards it yeah so
moving on to the next the protocol
negotiation so during the uh selection
of the multistream uh there are various
multi stream that the client can support
right so we need to negotiate over a one
particular stream so that's what the
protocol negotiation is there like that
what the negotiation of the protocol
happens between the peers when they are
you know uh trying to have a connection
between
them uh multiplexing so multiplexing is
used for creating uh multiple virtual
connections within a stream so it's like
a more scalable connection when the peer
wants to send the multiple stream of
messages over a
network so it comprises the implex and
the YX it's fast and
efficient now uh I would like to move on
to my contribution toward it towards it
so like I build the full implementation
the TLs protocol uh so I would like to
give the introduction of the TLs
protocol like TLS protocol is is a
secure lay communication used to encrypt
data and authenticate
peers um so how it works so basically uh
during the handshake peers uh auth uh so
peers are not compiled to use their
original public skill they can use any
arbitrary key types and um and
so uh like then PE then
the yeah peers use their Hy along with
the into the lipy to be publicy
extension so Li lipy to public extension
comprises of two parts that is the
signature and the host the host key so
uh for making the signature the end
points what they do is they uh add on
the string li2 is handshake along with
the uh uh now moving on to the perf
protocol so uh per protocol is basically
used to measure how fast the lipid B
works it's a benchmarking protocol and
it depends upon the two uh scenarios
like the the throughput per second uh
and also the handex per
second so yeah so like it comprises of
the two tests update update test and the
download speed test so what happens with
the update speed test is that the client
configures the server to uh
to uh to respond with the zero bytes
that means the client will send the data
and the server don't doesn't need to
send back the data so once the
predetermined bytes has been sent to the
server uh the CL the client closes the
connection so it means uh so yeah so it
closes the connections and the time is
calculated for the same and that's how
the upload speed test has been
calculated moving on to the download
speed test so download speed test uh the
client uh configures server to send over
the end bytes which is is like the the B
total bites that will be downloaded from
the server so
uh so uh then uh like the server sends
the data and it and um yeah and that's
the time is calculated and that's how
the download speed test is conducted so
here's the screenshot showing the time
seconds for the download speed test it's
like uh I haven't fully implemented it
it's in progress and like
um here's the link for the pr and like U
like uh the mentor has suggested some
changes so like I need to work it's in
progress um moving on to the next so the
next thing that I implemented was the
noise protocol feature Edition so um so
as described earlier that um um that
during the protocol negotiation we need
to select the particular protocol for
like the steam multiplexing right so uh
so what happens is like uh
um uh we can fasten the process during
the hand SEC itself like we can combine
the uh the handic and the protocol
negotiation as well so it can be done
with the help of the noise extensions uh
it's a particular feature of the noise
so like what happens is like the the
when so like there is the list of the uh
Proto list of the protocols that the
client um that the client supports and
there's a list of protocols that the
server supports and then when they doing
the hand sick so uh the follow will be
conducted through it and like the first
Common protocol will be selected and
yeah so here's the most we for the
same thank
you thank you so much Rose uh before we
continue with the second half of this
presentation do we have questions on
this part on Rose
anyone comments
questions okay we can slowly move on
then guys come come to please
plugin um yeah because the story of this
project is that it was started um as as
a joint um uh aort but then it split
into these parts so we have the C part
uh by rows and we continue with the
prism uh implementation uh with Kira and
Richa so yeah I promise you you will see
rich on the stage again today and uh
here it is
so yeah uh yeah we need to switch to I
there was a question for uh for Rose uh
can you estimate the time required to
develop in a new
language could you more elaborate the
question like what like the uh is it
like the compilation time or like to
develop a new language or
what yeah so I've been working for the
past receive two three months for
this yeah so I've been contributing to
it from the past three
months yeah thank you and uh if we can
switch to HDMI we have DS slides ready
very well yeah okay uh is it is it
yours just click on the
slh no because
it's is it from the computer
ah yeah okay I see what's happening it's
mirror yeah yeah please use the the keys
to change okay uh you need to need to
use the keyboard okay go ahead guys
yeah uh all right
uh yeah uh the project was initially
started as like as a collaborated effort
uh on our on like three fellows Maxim
like which did uh who did like most of
the implementation part he not here uh
today uh and then Rich and
me uh so yeah our part of the project
was uh um since like lipop is like B uh
used uh used by pretty much every
consensus clients uh so uh we basically
were trying to trying to uh eliminate
the uh eliminate the use of integral
Parts uh for M um yeah since uh I think
they announced uh announced like few
months ago that they were going to stop
the maintenance of Le P2P uh so yeah
this project was uh basically uh trying
to help RM team to enable this um yeah
it also it also enables U elimination of
the Redundant components from the goal
e2p which are not really used and like
it just uh it just covers a lot of
spaghetti code in the prism code base uh
and uh still not sacrifice on any kind
of performance and of course uh like
implementing this project would involve
a deep understanding of liid to be uh
Library uh yeah moving forward with the
project structure so what we did uh we
divided the project into several
segments since our main motivation was
to uh remove the rendant parts of the
lipit uh to uh have
the uh
to have the uh components of lip P2P
which were basically used by the prism
team so we started with segregating we
started with understanding how lip P2P
was actually working and how Prisma was
LE leveraging the lip P2P protocol so we
segregated the components like what all
lipy had and what all we actually needed
for the prism then uh yeah we REM we
filtered the elements and uh there were
sever Elements which could have been
reused as they were a separate
Standalone components and several uh
components which were to be implemented
or refactored so we filtered out those
and then the next step was to implement
the components in uh in the package form
in the prism code base itself and then
testing next we had to uh integrated
with the prism code base to switch from
the lip P2P protocol implementation to
the in-house implementation and next was
to do the performance analysis and
optimize the code for better performance
or at least the same performance as was
with the lip2 protocol so how we uh yeah
how we move ahead so first we uh worked
on segregating the component so as we
can uh we have we found out the main
components which were actually used by
uh lip pit to uh pris in lip b2p so
there were some components for config
for basically for basic configuration of
the uh Prisma to be basic configuration
of lip P2P for prism including the peer
or the address configurations or the key
configurations for cryptographic for
cryptographic connections then the
second one was main which was basically
linked with the config for actually U
making the connections or the starting
configurations then there were TCP
transport so there were two transports
which were uh there are multiple
transports protocols which were
available in lip P2P but the Prisma
mainly uses the TCP and not the quick
Proto quick protocol so we didn't so we
eliminated the quick transport and used
the implemented the TCP one then there
were TLS which was used by quick
transport again so and the TCP
so uh yeah next coming to the
multiplexers they were mlex and YX
implemented within lip P2P but the prism
already only used the yamuk so we uh
eliminated the implex there and then
there were noise which which were just
to be used as it is from the lip P2P
then there were several more components
upgrader piore the network and the host
apart from this the connection Gator and
The Connection Manager they were not to
be used or yeah they were small segments
which we just included it in the uh main
of the config and then talking about the
gossip sub the these uh these were the
events which these were the things which
could be uh reused or
eliminated
next yeah so how we uh implemented uh so
yeah we have the components ready for
the config Main and the TCP transport
then the yamuk upgrader and
other Pier store Network swam and the
host so gossip sub and the remaining
components were either they were not to
be used or they were to be integrated in
the final protocol as it is from the lip
B2B so yeah these are the main
components that were to be implemented
as we uh figured out in our first
step next we can move on how we are
going to move ahead so we have the
individual components ready the next
Next Step would be to integrate it in
integrate the individual components make
the uh smaller code changes which are
needed to actually test and connect the
uh different individual components the
next would be testing and benchmarking
and a complete protocol within the prism
and then finally optimizing for better
performances yeah thank
you thank you very much both of you and
we have questions for Kira
Richa uh about the prism peer
peer-to-peer go Library
anyone okay any comments questions last
chance oh there is sorry uh yeah is the
same one from before okay yeah anyway
anyway please submit online questions if
you are on the stream over there um
otherwise we will slowly move uh kir
continue right and we have Caleb as well
yeah uh just get ready
guys and um yeah before we before we
switch you can still ask
questions okay okay um yeah okay next um
next session we have the online slides
right yeah yeah it's next session uh
yeah so uh the next session again we
have kir uh St on the stage and Caleb uh
they've been working on the epbs
implementation andrin proposal build
separation and other important projects
in ethereum right now uh in ethereum we
embraced the proposed build separation
but it's time to enshrine it in the
protocol make uh protocol aware of uh
what's happening with uh actual
proposers uh and Builders and um uh
Caleb and Kira has been working on
implementing it in Nimbus and prism
client um so you might see that uh the
consensus clients are doing a great
progress on uh on uh guys you don't have
online slides
it's it's in the it's in the thing right
because you're not plugged in
here yeah okay okay
um yeah just waiting for slides
again and um this is going to be our
last session of this uh morning part of
the EPF day so
I guess uh we can wait for it a minute
um yeah again Caleb andira on um prism
and uh Nimbus epbs implementation the
seal Clans are doing the work uh pushing
for the epbs and our fellows were there
to help so
yeah here the slides
it's not Grand Windows I'm sorry guys
it's it's that's the wrong
one the the uh epbs it says there in the
yeah the one
before ah this the one perfect thank you
so much so yeah let's St is yours go
ahead hi good afternoon everyone so my
name is SKB I've been working on the PBS
implementation in the nameus consensus
clients so coming into EPF the
motivation for me initially was choosing
a project that touched on almost every
major part of the consensus clients
because I wanted to have a deeper
understanding of the protocol in itself
so we decided I decided to choose EIP
um it was it was proposed to be
implemented in Nimbus I mean prison
initially but there was need for client
diversity and to have it implemented in
multiple clients so so basically what
what thep does is basically
separating the ethereum block into a
consensus and execution into the
consensus and execution part basically
so and it has the mechanism for the
proposer to be able to choose a builder
on chain or on protocol which is
currently done off
protocol so it fundamentally changes the
way a block is validated by decoupling
the execution validation from the
validation So currently um Bon block
proposal they requested AR rout from a
third party Builder via VIA a relay they
submit a signed blinded beon block to
that trusted relay um the relay now the
relay is responsible for replacing that
ashree rout with the full execution
payload submitted by the Builder
basically the relay is responsible for
like selecting the best bid from the
Builder and and carrying out the
transaction basically so this project
was about um solving this trust issue
basically because um so we are trying to
like enshrine this currently PBS exists
but outside of the protocol so he's just
basically trying to enshrine it in the
protocol exactly so it it helps to
guarantee two major things right a
proposal payment proposal payment
safety propos safety and Builder payload
safety so it just goes to say if an
honest Builder submits um if an that
submits a payload and like commits to a
particular beon block it it should
always get paid or he or she should
always get paid and the build payload
safety just basically means um if if he
publishes a payload then that block
should become canonical
so um core changes and basically our
approach was basically just it's a it's
a research work that has been in the
pipeline for a long while so what we
just did was basically what I did
basically was like
implementing implementing it based on
the spec that that had already been
written right so the major changes were
like in the execution layer there was no
changes required so basically keeps the
execution people happy then we just had
to like um make sure almost almost all
the Implement implementation had to be
done on the consensus layer which was
like the two major parts was the beon
chain and the Choice Logic the
Choice logic is I don't know quite
complex really but um quite complex
because currently that's the point
that's the point we are at we at and
there's a new there's a
new so that's what we are implementing
next basically
um all right uh I'm going to expand on
that a bit uh so uh basically Fork
Choice uh when we started the project we
had basically two uh designs one is like
blog auction and Slot auction uh by the
end of the project uh whatever like
whatever built was basically replaced by
a new design uh thanks to Franchesco uh
but yeah uh uh I'm going to explain like
what uh what what we did during the U
cohort and like what the what was the
design that we are working
on uh yeah so basically this what they
uh what epbs does is uh separate the
consensus and execution validation of
the block uh which means that uh right
uh now right now the ethereum block has
only like 2 seconds to do like
everything uh like validate uh da or uh
produce the kcg proofs
uh so what this will do is like separate
the consensus block from the execution
block uh so for the first three seconds
uh like proposers or validators can
validate the consensus block and the
execution validation can be deferred to
like uh the the next 3 seconds of like
next slot of the the first 3 seconds of
the next slot so basically you will get
like 12 to 15 seconds to validate da
instead of one or two and then also
there will be like more time to produce
uh k
proofs uh since like consensus block no
longer carries execution uh this also
results in Faster propagation like
faster Network propagation and
verification of that um and yeah this
also ensures that consensus liveness no
longer depends on the execution results
or uh the block uh the finality of the
execution block uh one last Point uh
here is like any Builder can set the
floor for for the auction through P2P
Market space this like remains unchanged
uh like this is what happens right now
except uh this done like on a off-
protocol relay
network uh yeah so basically this is
what uh an epvs slot looks like uh yeah
so it is divided in basically four
intervals uh the first three uh the
first three second is when honest
valuators would uh gather sign bits uh
or like the sign execution pillow header
uh from Builders and then submit uh
submit the consensus block or uh we call
it sign beacon block uh with this BS uh
for the second part it remains unchanged
uh as it is right now honest vators will
submit their ATT attestations uh uh what
is like happening right now as well uh
for the third for the third interval uh
the aggregators would aggregate this
attach stations and uh the Builder brast
their uh full either the full payload or
a message indicating that whether they
are uh withholding the payload or uh or
yeah I think basically that's it for the
fourth interval uh some variators are
selected to form a new payload Timeless
committee uh uh payload Timeless
committee and which which kind of attach
to the presence and timeliness of the uh
timeliness of the Builder
payload
um
okay yeah so this is like
what okay
uh yeah so at any given at any given uh
slot the blockchains head uh uh can be
like a block can be like a block from a
pre previous slot if the current slots
proposer has did not submit a block so
in this case uh all the validators would
attach to the parent block and then like
uh the slot would go like without a
block uh in the second case uh uh it
would be an empty block if if the if if
proposer does not submit any Block in
the current slot but the Builder U sorry
uh if the if the proposer submits a
block in the current slot and the
Builder did not reveal any payload on
time uh and in the third case uh it
would be like a full block when both uh
both proposer and Builder would uh
reveal the block uh on
time uh yeah so this this was the
initial Fork Choice rule that we are
like uh basing our implementation on uh
this is called blog slot uh uh sorry
blog auction uh
uh blog auction design uh which uh which
contain like new payload and with Boost
uh so basically this is what our mentors
like tested like just two days before
the Devcon I think yeah it's uh it's on
that uh uh the the block here uh sorry
the yeah the block propagator here
assumes that it is tested on low local
interrupt and uh meaning like that a
proposer and Builder are the same
entities uh we have haven't like yet
tested uh I think on the P2P side of
things
yet uh but yeah for the future work uh
implement the new Fork design uh if you
have time I can like expand on that a
bit more uh yeah and then like hope that
uh more more clients join our uh imp
like more uh more clients join uh and
like start the impation for EBS I think
te has already started it and Caleb and
I am like working on Nimbus and I think
p and Teran have been like working Rel
relentlessly on the prison side of
um yeah um and then like update the for
choose consensus spec according to the
new design uh do the spec test and like
at last uh do the datet
testing uh I don't know if to have time
I think
uh yeah this is uh this is the new like
this is this is what a slot would look
like in a new Fork Choice Design This
was proposed by Francesco in the last uh
epvs um breakout room uh this basically
design uh sorry combines uh epvs fossil
and uh DS into a unified Fork Choice
framework uh uh and like it it will
support all three of this like all in
one all in one fork Choice which will
work for all of
them uh uh so it basically uses
availability committee so the pad timely
payload Timeless committee is like U
renamed to the avability committee here
and uh and like now it doesn't only just
votes on the timeliness of the blog but
it also votes on the payload and the
blob ability uh yeah while also
enforcing the inclusion list through the
through deadlines so if you see I think
uh it's at the 10th second uh if uh if
so there's like a deadline for freezing
freezing uh of uh including the ILS uh
on uh that's uh that's what basically uh
I I committee uh kind of ensures uh not
to Mo go more on deep on the fossil side
of things uh but if we include fossil it
also um it also touches the execution
side of
things uh yeah and finally a special
thanks to all the mentors P Terence and
TK and uh and the excellent EF
researchers who came up with a new
simple design barab and Francisco and uh
Mario and Jos for uh organizing this
EPF thank
you thank you very
much yeah thank you so much guys thank
you for the kind words um so before we
uh wrap it up do we have any questions
comments for Caleb and
Kira um there's a ton of stuff a lot of
work done on both implementations I
think it's incredible guys um and if you
have any thoughts about it any comments
any
questions uh feel free to raise your
hand and uh yeah okay otherwise I guess
we'll wrap it up here uh yeah if there
is anything I mean we will be hanging
around like like feel free to reach us
out thank you so much guys and uh yeah
with have this presentation um yeah
let's give it up again once again for K
Caleb thank you so much guys incredible
work and yeah uh this uh right now we
are finishing the morning block of
presentations for EPF day as you can see
it's very intense we have ton of
presentations we have ton of people who
did a lot of work in the past uh past
five months so it's very dense very
intense and we need to take a break so
we will be back at 2: p.m.
in 45 minutes
roughly uh so we have a lunch break now
and we will see you back at 2 p.m with
more presentations and then discussion
panel with some former fellows so please
uh come hang hang out uh come join us
later as well thank you everyone see you
soon
one might 4 5 6 8 9
Ole
Ro
back
I don't know
all right I think we can go ahead and
get
started welcome back to the afternoon
session of the uh protocol Fellowship
lightning talk project
presentations next we have a talk with
uh Jin is a late Comer to EF this year
but was actually able
did thank you than
you okay let okay CH sorry I have sleep
yeah yes okay okay
uh hello everyone my name is Jim M my
project for EPF C5 is gring on windows
so in the hood I will discuss the cross
platform engineering challenge for erism
Cent challenges the all my challenges
can sum up to one
sentence it is hard to run erism
validator on L
Windows oh this is my challenges table
we have a memory allocator program we
have a stack Overflow we have test
failure we have rest running program all
this just for special for
Windows we have a missing set matric and
we also have unsafe Windows API problems
we also have uh special user case for
like scre server or sleeping uh finally
I was found it was not e to run a
validator yeah okay let's see how we can
solve this
challenges first for J M loog uh the
work is solution we just disable it like
most uh rust
project in in favor we in fav fav the
windows building
locator the truth is G man lock is great
for Su side but neither lary for age no
better uh okay for the stockle flow in
debac mode in fact L is a big challenge
because for developers because uh it's
proven to you f debarking the uh workr
is which uh you uh solution you just
read
uh run the project in release
mod uh the direct reason for for La
stack overall is because there are big
unstack allocation in debug mode in R Le
P2P C stack this is detail of uh Coan
but if if I have enough time we we we
will come back so for lce challenge uh
have two Reflections one is uh we should
limited glob State staties and I recom
to use a top signon to manage other
signon this is a comper in R project yes
secondly uh uh you you you say in fact
we have a uh deep with the true
program lives in the Deep Library so you
cannot e fix fix age so we just use
overround uh I in the debarking of of L
stock ofall I I have reviewed the uh the
LA P2P Source C and relative source code
I found larra Mell engineering program
so I suggest uh we should resync and uh
even may we may overh hold our La uh fun
statement labor race like laor P2P
because it's really cral for our uh
horror
ISM uh cor
infact okay uh for the test failures uh
in the consensus spe test which is just
because uh
the uh uh default configuration of a
gate on Windows it uh it is uh
inconsistent uh the configuration this
CA the this where cost the gate on
Windows changes the the uh ssz
related uh T architecture so we have
this uh uh s uh T failure but listen
here is the uh uh consistence and
debuggability are so important for
developers Perman so lever over emphasis
on this topic I
think okay we come to the r running
problem in fact is also a big problem
because we need two two kinds of uh cl
cl in instance running your in your
computer in your load to form a a
validator yes so you cannot just
consensus Cent right the rest you in
fact uh uh we met program is a database
commit iro 98 in fact now M list on on
our C but in fact it still exist uh the
work around is just we run the rest in
wsr on Windows the G luse the wsr
windows can into uh interactive with the
uh La Windows uh s seemlessly but uh uh
the big program here is uh we are going
to an other inconsistent case we for the
same database file it can work the race
can work where on lus but cannot work on
Windows okay the lesson the lessons
there are two lessons here uh the one
the first the first is uh even for rest
uh it whereas it is a we engineered rust
project but it is still are not good and
ready for uh running on Windows okay
then the other uh uh Point lesson here
we want to it's a it's a big p want to
mention which is related to the of
problem is the backhand database engine
of R it is a uh mdbx uh K value edding
uh uh database but uh if we have enough
time we come back
here uh for the missing set Matrix I
just the work solution is I just
Implement what the missing so there are
two interesting point here want to point
uh the the first L is uh the SST related
Matrix in fact has it API propos found
the uh B CH in team so I'm I fe uh I
feel with our irism uh has a great
Community here yes the second Point here
is after my f the grounding now has a b
statement rated MATC support on Windows
than than Lighthouse you know the
grounding is a uh Miler CL but the
lighthouse is a major Cent
okay uh for unsafe uh Windows API we
just upstream and saves to the uh
Upstream libraries but the LI P for for
most rust project is the forbidden in
Lynch which we we we most of R team like
to use it but in fact just work the
codes in the current workspace not for
Row in the libraries
so okay for the uh uh the special use
case for Windows like uh let's noow prog
like the uh screen server and the
sleeping that's that's okay we can
recover from
age uh oh finally
we uh we come to the last
uh challenge I have is uh what I said it
is not a to run rism validators the
solution is for me is just to learn and
to try okay but I think this is this
maybe okay for uh for me like a great a
great development but this is not good
for come users I think so uh I so I
propose
way in the future we we can have a we
can explore uh great uh in Integra
integration of our current laid uh uh
Cent Yeah in our uh eras core
infrastructure okay finally I want to I
hope uh running a eras validator should
be should be less hard okay let's we
come to the S and
conclusions uh firstly rust is great for
cross plan cross PL form yes we we have
many different targets with support but
uh but for current situation we have not
fully re utilized it yet uh secondly
uhm C often share come Implement and uh
uh fundament libraries as we say the LA
P2P like but um if one have a bark in
the fundamental
libraries in fact in fact fun
engineering is not not good I fact up
the infrastructure World infrastructure
uh client so it's this I think it is a
defective for the Cent diversity
yes third
I think we can get more best practice
from wider uh rust uh
ecosystem uh finally I I think uh EPF is
great because I'm this C five EPF C five
so I me in this process EPF I'm began to
have a big have deep understand protocol
the development and the source code of
several our sever core CL cleans
yes uh fin as I want to say say big uh
thanks to myo uh
the uh solers from granding team and the
W wck day from gr sorry I I'm not
exactly the weekday l so N is a gate Hub
Candler I'm also appreciate Tim B uh he
introduced uh the EPF to me and also
appreciate the josia and the majoro
of EPF thanks for your great hype thanks
for all
your okay Le about me so I'm great in in
Ras database and JK andm cor
infrastructure so I'm if you guys has uh
this related opportunity you can don't
hit to to contact me thank thank you
that's my PR all right thanks a lot J
any questions for Jan on his uh project
on grandine support for
Windows all right uh next we will
welcome up BMA and zaratustra for uh
their
project are you guys GNA plug in
yeah has it worked yet
the other one I'm
gu are changes
all right let's hear it for zarra
BMA hi
guys so we worked on consensus client
performance profiling um both of us at
the same interest starting off with
grandine and branching out a bit as you
do during a work on a project um there's
a couple of repos listed there two of
them are on GitHub because that's where
everybody else is and I myself prefer
gitlab so I threw that in um I'm gonna
hand it to Bulma who's gonna take the
first bit hello hello okay sorry um my
name is Messi and I'm here to make a pro
presentation Although our projects
shifted a little bit um the initial goal
was to work on security and testing but
due to some um uh I underestimated some
things and so that was I had to focus on
um performance analysis a comparative
analysis between the consious client oh
okay so
okay sorry um so the initial goal was to
do a security and reliability of
grinding and then there was a shift in
Focus which transitioned to profiling
multiple clients due to technical
challenges which I
faced oh okay
sorry so um first of all I used so many
performance security um tools the ones
that stood out to me was a flame graph
and then it implementing a timing Matrix
on grinding this is example of what a
flame graph looks like uh though it's
not quite readable because you have to
like click in and then out to really
view but okay so looking at this Stacks
um grinding is actually performing very
well from the stock overview although
there's a drop down from the Tokyo run
time which is at
the key observation here is there is a
high Trend utilization on 97% in system
Trends and initial setup significant
drop down to 6 in Tokyo real time
operations and then a consistent
performance across Tokyo tax management
also multi trained worker execution
showing similar patterns in different
narratives is that
okay okay
soor so
um this is the T 5% looking at the
system trade level which is 97% then too
R time which is 62% and then the which
identifies 35% drop down so the there is
the the cliff there is kind of huge and
then the high performance areas are
trade initialization system level
operations and then Co Trend management
which gred pretty handled very well and
then performed very well in that aspect
so areas of Investigation okay a
disclaimer this is a research project
it's still I will call it a research
because so many can change and then I
might be wrong in some cases so areas of
Investigation for me is it Toyo TI
scheding which is at 62% and then the
signon time overhead and the tax
profiling
efficiency and um talking about the
timing metrics which I had to
implement for now I okay I was salus
suggested that I had to do a comparative
analysis between um um grinding and then
lighthouse so for now I only have the
data on grinded and I'm yet to produce
the data for lighthous to properly do a
comparative analysis but this is
basically how the data looks like and
I'm hoping to use this to do more
investigation on um formal to do more
analysis on formal verification and uh
Fen creating a targeted Fen and U what's
it called from M analysis sorry and hold
on
so the reason why I had to use tools
like the frame graph and the time um the
timing metrics was to help me understand
more about the conscious client because
I really
underestimated what it is to force and
build um a a forer for a consious client
because coming from a security a smart
conscious security background I thought
it was the same perspective but I was
proven wrong and the complexity was
overwhelming but using analysis and
using this data I think I be able to Pro
to produce and to come up with a
strategy in in order to have a for to
build a fora which has to do with um
making it a targeted fora and also for
formal verification because I understood
that um there is no much um um um um
work on formal verification on ethereum
consensus client thank you I can hand
over to you
so small premise to the second part so
as I mentioned we initially started
working on grandine there was a slight
shift in Focus transitioned to doing
this profiling of multiple clients at
least it was what I focused on uh devops
emphasis I decided to go there since I
have no experience doing any sort of
devops work and I figured having a
couple of months seemed like a lot of
time here that's a good thing to pick up
so that was fun uh build my own server
from secondhand part first time doing
that running prox MOX um pretty
interesting experience uh learned a lot
challenges Hardware constraints I
thought I had sufficient amount but
turned out that maybe the minimal
required specs listed are not entirely
enough to run all the notes on a single
uh server at least not the one that I
decided to assemble um and regarding the
outcome yeah well there's limited data
uh the results are modest but valuable
insight into running ethereum noes was
acquired so let me tell you a bit about
the setup that I used so I listed on the
left side for you guys the consensus
clients that are there and the versions
of those that I used two of them don't
have a version specified and that's
because I didn't manage to run run them
so they were not included I would like
to include them at a later stage when I
have the capacity to do so in terms of
Hardware uh I have dedicated uh eight
course to each of these um Ram 32 GB and
the dis space was 2 terab and vme drive
which is sufficient for all these
clients to not be limited there in the
setup I make sure that each of them
running on a virtual machine is also is
limited to this basically so there's no
way that if one of the VMS isn't using
all of its Ram that one of the others
could uh use it because it's available
this is normally something that Pro Mar
supports but that of course doesn't
allow for a proper comparative analysis
um then the services that I also got to
work with and got to learn about and Ne
mind of course I need an execution
client and a whole bunch of other
services that anybody who's done
anything in devops uh probably knows so
Prometheus obviously uh for metrics um
I'm not going to mention them all but I
will just say that these were a lot of
tools that were new to me um quite
interesting to see how many services you
actually need to run such a such a node
in a in a useful way so that you
actually can monitor what's going
on then I'm going to show a couple of
figures on some of the data it's a bit
older data but uh so be it so this is
the CPU usage that it measured over the
course of About a Week a little longer
uh for four of the consens clients that
I mentioned and now the question
obviously is what does this mean and I
don't have a crystal clear answer to you
I do notice some things which you'll
probably also notice straight away which
is a grandine seems on the average to
have the lowest CPU usage but there is
somewhere a spike in the middle um I see
that in a later figure in the next
figure I'll show as well and honestly
I'm curious what happened there but I
simply don't know I probably need more
data from the metric alone it's not so
clear it could be various things that
might be something Network related could
be something client specific simply
don't have an answer for you there here
I looked at the memory usage of the
different
clients um also you see the same spike
in the same period but it's for a
different client
so yeah there's definitely something
going on there and I would say that my
suspicion is that it has something to do
with my server at that point since I see
this weird pattern but like I said I
have no straight answers for you and
then the last figure
I will show I think this is actually one
of the things that is highly relevant I
only show grandine and Lighthouse here
because the initial Focus was more or
less on grandine and I well for
comparison also taking into
consideration Lighthouse so the number
of pairs that you have is obviously very
important uh and you can see that there
are actually quite large spikes to the
downside I have the limit set at 100 so
that's why it looks kind of artificially
kept there um but yeah if you lose a lot
of pairs that's obviously not a good
sign and well it seems to be
fairly maybe not stable enough um or
another stable as you might want it so
in terms of
Outlook what I would like to do is fine
and skilled this node management so I
now have almost automated setup for the
provisioning of my VMS I would still
need to switch to nixos or Taos to have
it fairly immutable and item potent then
I want to integrate e Docker this is
something that I simply didn't have
enough time to properly look into so so
far what I've been using is sge uh well
I started with simple scripts and
running local binaries and eventually
went on to use Docker compose but yeah I
would like to move to kubernetes and I
would like to also explore e Docker
because I think it hovers some of the
things that I was lacking or missing in
sge and then I only very recently that
will that is to say yesterday learned
about the secret shared validator um so
I'm going to look into this this is
something in the direction that I was
thinking I want to this setup I want to
use this setup myself with an agentic
system because there I have a background
um so I think it might actually partly
at least overlap with this could shared
validator setup because I was thinking
in that same direction and lastly I want
to optimize the data queries because the
idea of this agenic system would be to
integrate those metrics so that the
agent can proactively take the necessary
measures for example manager node if you
see that your peers are getting too low
maybe you need to well do something
to um to fix that um yeah that is all I
have for you so then I only have a thank
you slide left um I'll obviously Mario
and Josh it was amazing thanks so much
for uh well all the time and effort that
you guys spent and granting us this
opportunity to be part of the
EPF um yeah I think that's it so if
there are any questions
then uh hey thank you for an amazing
talk uh I was wondering if the notes
that you ran were validators or just
notes no also validators also validators
okay so it might be uh nice to see the
difference between like a for each
client what's different if they're a
validator and if they're not yeah I
agree there's lots of experiments you
could do um really really lot more I
would have liked to have done more of
them but it's as BMA already said it's
quite overwhelming because even when I
thought I had all the pieces and sort of
working and then you need to learn
promql okay and then you learn a bit of
promql and then you see all the metrics
that are there for example grafana which
I also use a little bit at somebody I
just decided okay just canot do it so
yeah I agree there's lots more um and
those those things would be interesting
to look at yeah amazing thank
you any other
questions
Mario um yeah um yeah I was wondering
because you mentioned the setup with brm
and you want to automatize it set up
with NEX it's really cool uh I'm just
did you didn't really have in slides
like some more about it like for example
what are what virtualization are you
using with prox MOX like uh KVM or lxc
or yeah KVM is what I'm using but I
could explore maybe Alternatives I
haven't explored any alternatives there
so you think it's worth exploring uh
just the Linux containers are good for
servers as well uh and I was wondering
like because procm supports both and you
just yeah you didn't have the details
there so I was wondering like which one
no this is true no this I use it's the
lightweight version right so for Linux
that's a that's a preferred option I
would say yeah cool thanks so much much
here all right thank you
guys next up we have another
presentation from Hong Leong uh and his
presentation will be on his work on the
pios in Grand as
well let's welcome H
Leon thank you intruction
so my project uh is working on improving
p in crin um
okay the wrong
one oh
okay thank
you okay so the motivation behind the
projects uh because there are uh some
interesting uh topic and uh uh chain
that uh proposed by is the client team
um like why I use uh why I pick up p uh
to be working on and why I uh choose a
cred in to contribute to so because like
uh at the time that I start the program
P do uh was considered to be included in
the Petra so they have like a specific
deadline and also uh there is a active
research and development and and also
like a running down net that uh uh uh
all client team like working on and to
implement the the P so yeah but uh due
to the like uh the daret instability and
the interoperability uh issue that
happening in the previous Steel Net
that's why it move into the faka the
network upgrade uh after the Petra one
and also the uh uh um the gring is right
and in because I'm also um familiar with
the programming language and I in then
you might ask like uh Lighthouse also
writing in Dr as well but why I didn't
uh contribute uh to Lighthouse anyway
but because of the last uh point because
scin is a like a new client uh cons
client and the team is uh like a small
on like only four people specifically so
they don't have like uh capacity to
handle different thing yes so that's why
I would like to contribute to to them
yeah so a little bit of a Contex about
the pasas so pasas is like um the uh
networking chain to um the protocol Ean
protocol which allow the theage sampling
and appear to do datab sampling without
like to download all of the uh blob that
uh in in the network to ensure the blob
availability just like uh download just
a few channel so now like without P so H
node have to download or and and to
participate in the gossip shop the
subnet so all the all the block like in
in the block that I have like six Block
in it so they have to download all of
them like that there's no like theab
sampling in there so then if like the P
activated so like uh the node like only
to participate a few CH of it uh um in
in in in the gosip St so they don't need
to download the whole the BL data just a
future like like in the diagram
B so the project uh progress so just
want to mention like uh before I start
working on P like graning team had have
done the ground work um already already
joining the the first interet of the
pust and uh um because like the spec is
uh involving and changing so like uh
they like I mentioned earlier like the
team don't have the capacity to handle
of different things um and and then I uh
join to contribute to and so I make work
as uh some
uh done some work on and then ready to
join the p d to so there is a spec chain
into like switching from the peer dust
um uh uh peer sampling to the subnet
sampling so it is kind like a simplified
version of the PE PE D so uh like a PE
dust without peer sampling so they don't
like uh uh um get it a complexity of the
the PSM PL
yeah and also another spec chain into
the D 2 as well so there's add the
metadata V3 so it is kind like a OBC
method like it is just a simple like add
a new field into U the metadata like the
costet count so it is small thing and
and fun fact that uh the D two didn't B
up where so it um there's a back in CR
that uh cause like um theet uh the the
whole D to be like messed up so I'm the
one that mess up and also at least like
we found that the H case that uh that
that spec is missing like the check in
in the the colum S car with the zero
commitment
so uh and then at later on uh we uh
relaunch the dnet the D Tre so there is
a uh P request and the chain to the SP
to include the the validation on the uh
C block so yeah at least we found that
the the H case on that and then also
like the Destry also didn't went uh what
as well so there's um like a re from on
the prism that that um like uh re around
like 100 uh, of slot so there still some
of like uncertainty about the real uh
root cause of the issues yet and mainly
maybe like around the peers uh scoring
yeah and the red limiting stuff and
after that uh like uh I found some issue
regarding to the byang request and
response um and also like some of the uh
sing synchronization issue that
happening um previously like uh CR like
cannot synchronizing from
thees yeah and then uh previously uh
they don't uh gr didn't have like that
uh reconstruction handling so I uh try
to make sure that uh the Reconstruction
is have been implementing before the D
net fall and then I um implementing the
rebase P onra so it is like um repace so
the team uh there's a spec chain in uh
in the D 4 so and another one is the
engine G blob one but it's not a danger
so I'm working on it uh currently and
there are several like open p okay sorry
uh there are uh sever several open p
might be included as well in one of them
uh uh the the L metric uh from B fellow
here is SCA you're working on
it
oh so the next step I will like continue
and contributing to the project until
like it with be launch and continue
implement the uh engine get Blob B one
it is the
uh optional one but I I most all the
client are already implementing uh this
engineeri and can you facing back
because there are some few B that still
um in in the implementation and also
continue improving the performance clean
up the C and yes and there will be uh
later later on like d so I will keep up
with the spe chain
so the challenge is that
um it is the the the most challenging
and experiencing uh part for me to
implement uh the p d is mainly like to
to understand the concept of datail
sampling uh particularly in the P
because like it's not um the uh the
important thing is not uh um the hard
part is not uh the implementing like you
try to understand need to understand the
concept and also like to get yourself uh
familiar with the structure of the
codebase cring and also all uh all the
clients because uh you need to uh
understand the behavior the correct
Behavior like when you dip up you will
like find some of like um like a uh
wrong Behavior or something that and
then you you you need need to um uh
check with all the client
implementation uh like what is their
their behaviors and then uh try to
interop yeah and my experience in the
EPS yes it it's great t for me uh I
before I join EPS I did not uh
understand anything about the core
protocol development and I don't know uh
about the decision that made into the
protocol train and it's a lot and I I
might say I I'm a permission L
participant and and I uh it is a great
opportunity like for the public to uh
would like to uh uh exploring the core
protocal development is is completely
permissionless
yeah yeah thank you thank you everyone
all right thank you any
questions got one question over
here yeah so couple of questions what
did you use for your testing setup uh
for Pierce and another one is uh about
uh what was your uh networking
stack oh oh the the for the testing uh
we use a CIS for the local testing ones
so we spin up uh a few note like from
the different client so then we we can
test the interoperability between
clients
yeah for the networking state is the
same like you just go to see and we add
some of the um service to to um doing
the mentoring uh monitoring the and the
dasb and some and the metric think
yeah any other
questions awesome thanks again
Ang all right next up we have
presentation from Kya who will be
talking about her work on displaying
metrics for beiras across a few clients
all right hi everyone my name is ska and
my project is pust metric
specifications um so um um the slide uh
I'm not going to dive into what is spear
do uh it's been it's been mentioned
already so it's scaling solution for
data availability and um all of you know
that uh ethereum protocol is built with
u um according to the consensus uh and
execution specs um so uh but the matter
different clients are different I mean
uh we don't have uh the unified metrics
for the
clients um it's challenging for ceps to
compare the different clients uh and for
for the devops team it's chall to um to
monitor all the clients there are a lot
of
them um and it's also difficult for
research to uh get the unified data to
synchronize the
data so the solution is One dashboard
for all clients and uh the unified
metric
specs uh so how it worked
before uh we
had sorry we had uh different dashboards
for every client uh different graphs uh
so uh to compare different clients uh it
was quite painful
uh what do we have now for
pias um we have a single dashboard with
different clients um the metrics
implemented for grandine for Lighthouse
for um taco and
um grandine Lighthouse Teo and prism a
little bit so um uh for this dashboard
you can apply
filters um you can choose if it's a full
node or super node um you can uh choose
one client two clients all clients and
you can also choose an instance and
compare or monitor or whever you
need let's look at some examples um uh
this blue client implemented some
feature this one is piras and uh uh the
filter is for full nodes and it is
obvious that there are some issues with
it because other clients um for full
nodes they have less values and this one
looks like super
node this is the case for
investigating
uh so the next one is obviously uh there
is some issue with one client uh which
devops can catch Vis visually or can
um can apply the alerts Grana alerts as
well um for researchers the this uh
dashboard is helpful because they can
build benchmarks uh for example here you
can see two clients and they have the
same metric uh so this means that
probably the case The
Benchmark with specific um
numbers uh this dashboard is also added
to curtois so um whenever you want to
run curosis and you need a beard
dashboard it's already there so now need
to build your own
dashboard um the dashboard is already
actively used by Jimmy by Sigma Prime um
this is his notes about um no pust notes
uh super noes and full nodes bandwidth
uh and you can see the graphs from from
dashboard why hasn't it been done
before because um especially before the
merge there were not so many clients and
it was manageable but why do we need it
now because the ecosystem is growing and
there is more and more clients not so
fast but anyway the uh number of clients
grows and as Perry told me plus one
client is 10 time consuming
for monitoring for testing
Etc so the benefits of metric specs uh
it's easier to catch errors um it's
easier to save resources of teams so you
just save some time or um for uh for
building for testing Etc um it's easier
to compare clients which sometimes
really important and for researchers
it's easier to create
benchmarks um currently perod metrics uh
specs cover um the main points of pias
uh they computation inclusion proof uh K
verification reconstruction gossip
verification and
custody and uh they also um uh in the
discussion gossips up and lipop metrics
with labels for P
does uh this is the current state of
project uh some of the metrics are
already implemented uh some of them are
open PRS um some they need a review or
need an adjustment and I hope to cover
all this um table but that's not the end
the road map uh sorry the workflow for
the implementing metric is kind of like
this we discover it on um we discuss it
on uh perod breakout rooms uh then uh
put it into the GitHub PCS and then
develop test and uh if researchers need
then um create a
benchmarks uh then process of reviewing
goes and if it's needed then the circle
starts again and if it's not it's
merged uh but this is not the end so
puras metrics are only the beginning I
would say um the next goal is
to uh spread it to uh consensus clients
and later to execution
later um I would like to wrap up with
the words of a poet such an iin client
diversity for the win uh so um this
project um presents uh ethereum values
uh such like client that University and
collaboration and it wouldn't be
possible without my mentors Perry and
Barnabas from E PES and Dimitri from te
and Jimmy from Lighthouse I also worked
with different teams uh prism with Grine
uh I had a feedback from megalops um
they are
researchers um so yeah I'm going to Pro
proceed with it
thank you for your attention if if
you're interested in any details you can
scan the code and thank
you all right thank you Kata any
questions for
Kata thank you Josh and Mario that's
what
wonderful I may have missed it but how
are the metrics being collected at the
moment is it being produced by curtosis
um if you run it on cortosis yeah they
are collected uh using prus matrix it's
like standard stuff yeah but devop team
also has it their own databases on
Victoria Matrix they collected there so
these um specifications are more for
internal use uh for devops team for
researchers but they also available for
uh public usage
uh you mentioned CL specs do you know if
there's any work being done on that yet
uh no no but we we're in discussions
about uh uh building specs for um lipop
and gosip sub because yeah because they
are already in production so it's
difficult to kind kind of synchronize
them but we're trying to find the
solution
uh how do you think would it be
useful yeah you find
it have lots of lots of lots of metrics
um and the teams also have there lots
and lots of metrics so finding the would
be the easiest
just having a set that everyone
stand this is not a m so um teams can
also uh create their own metrics if they
need like more or in different ways so
but some of of some of the points uh
would be great to be synchronized in any
points
yeah so um is it correct that uh you uh
implemented these uh PRS in different
client teams like in different clients
or did yeah so do you see this moving
forward to add these metrics across all
the parts of the clients do you see this
as something that you or the devops team
would do or is it that the client's team
themselves have to implement these um
like going the way how I see it so we
will start we will get used to it and
when the new for example EIP or any
other feature is delivered later which
is not yet started started the process
is for example in
research um with building the specs for
the clients we can advance at least like
the we can mark the point where should
be the metric and the clients uh during
the building the feature the IP uh they
could bring bring it right into the
right place so I I hope I won't do all
the metrics for all the client
so the idea is just uh to bring this
process so it came natural and not that
painful for teams all right thank
you all right thank you Kata thank
you all right next up we have a
presentation from Glory will be talking
about her project over the past 5 months
uh Integra creting um emary test net
into teu and
Basu hello everyone how you guys doing
today I hope you're having a great time
at
Devcon okay so today I'm going to be
talking about Native implementation of e
test net on client p and Bess um first
of all I'm glory and um I would like to
ask this question how many of us have
heard of the world a test before
now okay so those people that raised
their hand of are people in the
fellowship so that means we still have
so many persons that are yet to know
about eory I remember when I started
working on this project um my mentors um
the guys from Teo and B were like what
is aary I've not heard of it for so I
had to start explaining I had to share
the link to the documentation for them
to go through it okay so with that being
said
okay all right so why they're trying to
work on it um first of all
what is Emory em is a resetable
shortterm based in tesate so when we say
it's resetable it gets to reset itself
after a given period of time so it's not
like the regular test net that we are
used to like seoa or ly and the rest of
them so is kind of dynamic and not like
the others like I said and um so I
wouldn't really want to go deep into
Emory so I would just be diving uh into
what I actually worked on
so yeah for the project uh the goal of
the project was for me to implement or
create native support for e on besso and
Ciro clients and why was this needed we
realized that just like I asked and most
of you didn't know about fmy so FM is
still pretty new and we are trying to
see how we can get more clients and more
people to know about it and to start
using it so we want to improve the user
adoption and um we want more clients to
be running on a natively without having
to go into the manual process of setting
up so based on the work that I've done
um initially when I started this project
I noticed and there are some work that
have been done before in the past by
some other fellows and um uh at that
time we had support for forget red and
Lighthouse and lower so work has started
for this client uh I don't think they
are completed yet but uh as at the time
I came in I noticed there was no native
support for Teco so for that we needed
to download the the Genesis uh SSD files
and the Genesis files and all of that to
do those process manually when trying to
run the node So to avoid that um EIC we
decided to implement native support for
it and what does that really mean when
we talk about Native support we want a
situation whereby when you are trying to
run a node you can easily do something
like B D- Network emary just like you do
with other supported clients like SEO
and the rest so um and because eir is
quite different the process of
implementing it on clients was not that
straightforward like with other clients
so we needed to do things like um trying
to write some custom functions or some
custom classes as well to be able to
like um work on the updating of the of
the Genesis State and all that so
because we understand that AEM update
after a certain period of time so there
were need there was need for a custom
function to be created for that to be
able to happen dynamically so these were
some of the um things I did and I okay
so for the Genesis for the affirm
Genesis 5 I noticed um the way it is
structured when I and checked the
original repository was different from
what best two client was expecting so I
had to make some changes like um other
things like the S as the discovery noes
and the boot noes to the to the Genesis
file itself and also I updated the
documentation when I'm when I was done
with the work so that when you go to
their documentation now you can see just
like you see seis on their dock you can
see a there and know the right command
for you to run to be able to do that and
with this we also implemented some
testing because every aspect of this
project um with the way Bessel client
Works everything you do you have to
write test for it so every
implementation work done here was also
properly
tested and so for Tech we did similar
stuff um but with C you know um for the
C clients we don't use the noral and Jon
format file for the Genesis so it was
SSD and um it was it's not as easy to
manipulate as you do with um the Genesis
format five so with that we needed to
take a different approach and see how we
can load the ss5 from the e repository
to get the updated um States at every
given point in time so the same thing
applies with te we also created a custom
class and function so we able to update
the the Genesis States at every given
point in time dynamically so if you're
running the node you're able to get the
updated state every time so you don't
need and also with tech I worked on the
reset feature which which means uh if
you are running a node you don't have to
stop the node manually to go delete your
database and all of those stuff when
it's due for reset it reset itself and
then deletes all the necess FES that
needs to be deleted um in the
DB yeah so um pretty much like I said
earlier so the there's no much
difference between the implementation on
Teo and
just some different files um and changes
that were made which was not exactly the
same thing and then the SS um file I
mentioned and Genesis file and all of
that okay so yeah so this is like a
summary of everything I did on the
implementation for Te and B I completed
in the initial research then created
custom function for eir on both B and
the TECO clients and then right now you
can have now right now we have native
support on both clients so you can use
the use the flag the native flag like B-
Network FM or te Network F and you can
do that as well and then the
documentations are also properly updated
and yeah we have suitable tests for all
this so in summary um for the whole
period of the fellowship I was able to
get up to 50 PR's Meed and currently
have one PR that I'm still working on
and one open
issue so as far as work in progress goes
um based on what I plan to do during the
course of the fellowship I I wasn't able
to get to the point where I um was able
to work like work on the reset feature
so right now I still have the reset
feature to implement on B and then on as
far as the website goes I'm still yet to
work on that like just like redesign the
website and the tutorial work through
which I plann to do but I wasn't I
didn't have enough time to do that
before
now so yeah for the challenges these are
some of the challenges I faced so uh
this was my first time working on
protocol development so I before now I
do normal application level development
like writing smart contract doing dabs
and all that so this was my first time
and it was a lot of learning process had
to learn about the protocol CL eel and
that was one thing I love about this
project because it gave me the
opportunity to learn about those
different areas and then um trying to
understand the code base for each of
these clients was another big problem it
wasn't even writing the code itself but
going through their code base trying to
figure out where my implementation is
going to be those were some of the
challenges I had but thankfully enough I
had good mentors who were able to help
me navigate
that so for the future of the project I
plan to continue my work on the best
client to complete the reset fature and
um to also Implement there there's still
some little work I need to do on Teo for
the um validator clients on slots
processing and then to expand the
support to other clients so right now we
have some clients that don't U currently
have native support so my plan is to
continue working and to extend support
support to those other clients and also
to become a validator an eiry
validator so key takeaways for me um
this program has given me the
opportunity to dive into the ethereum
protocol itself which was one of the
reason I joined and then I've had the
opportunity to learn a lot about um what
the protocol entails client
implementation what the different
clients we have in the ethereum protocol
and all of that and then um before now I
do Java Java was actually like my very
first programming language when I
started but I've not done it for like
more than four years or so but with this
project I to come back to Java and I was
able to like improve on my skills on as
far as Java goes as well as testing and
all that then for EIP I've not actually
implemented anything working with an EIP
before but this project actually gave me
the opportunity to write code following
an EIP so that was that was really good
for me
yeah so for my mentor special thanks to
Paul Aris from the Teo team
Unfortunately they are not here right
now and then thanks to Sally from the
besto team thanks to Mario and PK 910
from the eem team and um the past fellow
OE uh when I started I asked her some
few questions and she also shared some
of her past U updates with me which was
really helpful as well and thanks to
Josh and um um Mar once again for the
opportunity and for setting this old
Fellowship Fellowship
up so before I go I would like to
encourage anyone here that want to that
is considering contributing to Emory so
if you want to do that you can do that
in different ways by you can be a node
Runner you can Implement on client like
I mentioned earlier we still have some
client that need implementation and then
you can if you're a dab developer you
can deploy to you can deploy to F test
net on your JB and run and test and see
how it goes and if you run node you can
also run sorry if you run infrastructure
you can run an infrastructure and if you
have any idea or whatever you can
communicate with us and uh to learn more
okay so about this so there's currently
an incentivization program on E that U
gives you an opportunity to contribute
and also get incentivized for it so um
okay let me not be fast to remove it so
in case you want to scan it you can
quickly scan it before I remove the
slide all right uh so yeah so this is
like summary of the work that has been
done and what is yet to be done in terms
of client implementation so the that's
for E client and then this is also for
the C clients and for resources if you
want to learn more about eph you can
check .ev and to join the community you
can check the E metric community
and thank you very
much all right thank you glory any
questions for Glory about her work on
the ephemerate test net
implementations I wonder is it something
different to become a affirm
validator no no it's not the same way
you are validator for other um Network U
you can do the same for family so you
still need 33 so I didn't get us you
still need
okay anyone
else yeah not really question just a
comment like huge thank you um I mean
many many thanks for all your work on a
family like it's a incredible job which
you've done uh implementing it and fully
merging it and have it usable in both
clients it's incredible thank you so
much for mentioning all the resources
the incentivization program and um uh
yeah like anybody can run validator
because being reset uh every month F has
infinite eat for you so we can just run
as many validators as possible maybe
even more than main ad at one point uh
but yeah thank you so much Glory we had
we F was created basically at Defcon two
years ago and now after two years it's
amazing to see such an amazing job being
done on at work so yeah thanks
again infinite eth you heard it
here all right next up we have a
presentation from Daniel on his network
simulations with
Shadow all right let's hear for Daniel
hi everyone thank you um my name isic
and I worked on Shadow NE
relations yeah
um the usual clickup problems
thanks um yeah so in my opinion Network
simulations are awesome because they
allow testing changes without rolling
out to public death Nets or even test
Nets which is slow and takes
synchronization and time and um small
local definites are possible but with um
a software called Shadow we can actually
run huge simulations with thousands of
notes and also with ual clients like not
any um like code written dedicated to
the um simulations so I kind of had a
three-phase project first I wanted to
prepare a tool that allowed me to easily
set up these Shadow simulations with e
theum clients um that is easier said
than done because Shadow can be kind of
finicky sometimes so it it took a
while um yeah in the second phase um um
I want to run experiments uh
specifically on pias and I don't want
also known as gossip sub
maybe if I saw in the experiments that
this is actually a useful tool um I
wanted to polish it and actually did uh
and S thus uh e Shadow was born um so to
sum it up there are like two main
artifacts from my project um the
experiment results and E shadow myself
in this presentation I will kind of
focus on on my results from pias and
afterwards uh also show you slowly um or
quickly um e Shadow right so pias um I
won't have time to explain pias fully so
I'm just going to assume there's uh some
awareness of what that is um few points
it's a update to scale BL blobs and the
basic principle is that we want to split
the blobs into Parts which we call
columns with every note taking custody
of some of those columns and there are
some Network CH changes required for
this to make sure that the notes um
properly um distribute uh The Columns
across the network and also so-called
supernes can reconstruct the blobs if
they have enough columns and right now
there's like 128 additional subnets um
in Gossip sub uh which all need to be uh
covered by when a note wants to proposed
right we need to get every column out so
yeah okay so I decided to run some
simulations to help the search and
development effort um keep in mind that
all results that I present here are like
a few months old at this point so the
situation might be uh better at this
point so my simulation setup involved
Lighthouse valid on each uh with 4,000
validators and in each experiment I
simulated 45 minutes of simulated time
um for a large simulation like this we
want to run on some server that
sufficiently large the the simulation is
fast enough for us and I kind of
experimented around a lot to make sure
to get like get nice performance going
kind of settled on a certain instance
type um you can see it there if you want
to run your own simulations with this
size
and my simulations took a round of 4
hours per like 45 minutes of simulated
time and so the the cost of per
simulation came around at uh
$10 right um but if you want like also
run smaller simulations we you can also
do this on your laptop like you don't
need such a b server this is just like
my setup for the P
simulations um so first I just tried
running it on the current implementation
in
Lighthouse and the simulated networks
quickly fell apart because they lost
sync and cannot really sync back up
again um and that was kind of similar
What was seen on Def Nets at the time uh
across basically all clients um so I
thought about how do I measure
performance and I had several metrics
but right here I will focus on what I
call score uh which is how many slots
can um over 66% of the network uh stay
in sync and with the
unmodified uh client with the default
configuration zero like immediately
after we posted blobs to the network um
we lost syc um but I also noticed that
if I designate every as super no so
custody all the columns not just uh
eight or four um then um the network
stayed stable until the end so I had
like the suspicion that something is
wrong with um the distribution of uh
columns when we just you know don't
broadcast all the columns at all time to
everyone and also that might be
something with super node reconstruction
of uh the s might be
wrong yeah um so and yeah the
investigation showed that the problem is
that if a node could not send all the
data out it would not retry that at all
causing some columns to get lost and
custody nodes uh for those columns would
just not deem the datas available and
would just not accept the block and lose
sync and they could not catch up I guess
the super notes were enough uh not
enough for that so how do we fix this I
did some simulations where I varied the
specs and um I did a lot of different um
variations I will focus on three here uh
we could increase the number of peers
because if we have more PE the
likelihood that we actually cover all
the columns uh when proposing a block um
with our Piers uh is higher so yeah the
base back or the base configuration and
Lighthouse uh looks for 100 Pierce 150
Pierce scored 10 with the metric I just
mentioned 200 uh already uh 56 and 300
surv the whole simulation so we could
see that confirming our Theory this
improves the
situation next up I increased the number
of columns covered per each Pier which
should have the same effect right
because the total number of C columns
also increases with uh changing this um
inre in from 4 to 8 scor 10 and 16
already survived the whole simulation
finally uh there should be Anem Emoji
here the the you know the the diagonal
face um right uh increasing the number
supernotes which should reconstruct the
um full blobs like all columns if they
have half or more um increasing to 10
scored zero to 25 of thousand uh right
score two and 75 also too and at that
point I didn't test any further because
I want to kind of keep it in a somewhat
realistic scape and I'm not sure how
many supernotes there will be but we
shouldn't in my opinion set the
Assumption too
high all right
um if you want to know more about that
um there are my weekly reports where I
got into a bit more detail so let's uh
move on to the state of each Shadow like
the tool I developed um because seeing
these simulations um I thought this is
hey this kind of useful I have like an
iterative approach and can evaluate
different configurations so I yeah spent
some weeks in the end to make it
available for everyone and the sources
is available on GitHub at ethereum
Shadow uh right now there's like first
class support for GE and Lighthouse and
there is some experimental uh support
for reth and
tiu um there is some documentation left
to be written um for basic use cases
there's like good uh documentation
already so you can check it out um and I
think in some cases usability can be
improved um but the hard part is that
shadow needs a lot of work for all
clients to work in
Shadow the reason for that is rather
technical basically
we need support for more Linux system
calls in shadow um and this is a lot of
effort unfortunately I um kind of won't
have much uh time so I hope that in the
coming uh weeks and month I can at least
help on the side a bit to maintain this
but I hope to also get the attention
from from the core defs to help me uh or
help us all um hopefully soon have each
Shadow for all the
clients okay uh there are some thanks to
be said uh first first of all thanks to
my mentors Aran Manning from Sigma Prime
and pup from etherum Foundation research
and also thanks for to J oliviera and
Jimmy Chen from S Prime who supported me
with some of their time and thanks to
Anon nurv from consensus uh he uh spare
headed the Techo support in a couple
last couple of weeks uh thanks to EPF
organization uh to Josh and uh Mario
thanks to the Ean foundation for hosting
the EPF and uh thank you for to all the
fellows it was was really Pleasant uh
and great to work with you all and
finally I'm going to plug my talk
tomorrow or our talk tomorrow um
simulating an ethereum Network scale
which will go more into detail on how
you can run these simulation yourself so
maybe see some of you there on stage one
at 10 1 p.m.
thank you for your
attention all right any questions for
Daniel
um hi just a couple questions one is um
when you're making all of these changes
to do these simulations are you actually
like updating the client code or is it
kind of like Network configurations um
yeah this is like the great advantage to
change these or to test these changes I
actually can change just the client I
don't need to develop anything just for
that I changed the actual client but for
like the things I showed you I think I
only needed to varry the configuration a
bit right but I also did some changes
that actually tried to fix these
underlying problems uh in L itself cool
yeah and then I guess my other question
um so are you starting simulations from
like a Genesis state or is it like a
fork of main net and can you do Forks of
like existing
networks well um I I start from a
Genesis State my tool suppos this it it
just does this for us uh which is nice
and forking main net I mean I'm aware
that there are Shadow Forks they are
unrelated to the name of uh the
simulation tool um I haven't tried it uh
the problem is that the Genesis state is
quite large and this might actually be
really hard to simulate like on a single
uh node right we have all the simulation
running on a single server and that
might be hard to have multiple clients
in parallel uh working on that so I
guess it's not really feasible
so it's wondering like once the
simulation completes can we see like
consolidated metrics on the Node or
something logs something like that very
good question uh this is how I evaluated
uh those simulations um I kind of forgot
to mention that here um the
simulation um you it is really really
easy to add a Prometheus note in the
simulation configuration
and that automatically gets configured
to pull the metrics from all the
lighthouse clients and in the end we
have like one huge Prometheus database
which allows us to just pull up some
grafana uh dashboards or any other
analysis also we have all the locks so
we can also look into those if there are
any like specific nodes that seem to be
acting strange or something
yeah in
can you can
right um You can also like run a note on
a data directory that has been generated
by the simulation afterwards it's a bit
weird because uh the simulation time
always takes place in the year 2000 so
it's really old data to the node if you
start it right now but you can run that
note and you can attach like a block
Explorer or the Explorer for example by
the the P Ops Team to it it's just a bit
wonky because of this timing
issue yeah
basically the simulation Shadow starts
the simulation time always at the 1 of
January in the 2000s so every time you
do a Prometheus query or um yeah look
into the block Explorer you have to keep
in mind that you have to act as if it
were the year 2000
yeah any final
questions there are some up there oh we
got one more uh how is it different from
curtosis okay in a nutshell uh
curtosis um you cannot run networks at
of this scale on a single server with
kosis because kosis is not capable of
like pretending to the processes that
time is running faster or slower than it
actually is basically like um having a a
separated simulation time from real time
um yeah I would say that is the main
difference that you can scale way higher
with uh
Shadow all right thanks Daniel
all right next up we have another
presentation from hoppenheimer and
chirag who will be talking about uh a
the rated list
okay uh hello guys uh thank you for
coming for the presentation um okay I
think I buy m so uh we are presenting
the rated list uh we were mentored by
danrad uh on this project it was his
idea originally uh what we did was uh to
formally specify the rated list uh take
it out from a Blog and have a proper uh
set of uh pythonic specs uh for the
rated list build a simulator uh which
against which we can uh test the rated
list uh write some unit tests so that we
have some basic sanity checks of the
rated list so that it does not perform
worser than just randomly pure sampling
uh and collect some metrics against uh
known attacks uh on existing dhds so uh
just to give you a quick intro to rated
list
sorry um so this was the idea proposed
by danrad um which is uh instead of
having a DST uh you form a tree of all
the nodes that you know you ask for uh
in kind of a pure exchange manner you
ask for nodes of the nodes and you build
out a tree and uh you start your peer
sampling and based on the peer sampling
uh if a node replies or does not reply
uh you propagate those scores back to
all the ancestors right so as you can
see the red node is a malicious node or
a node that does not respond and because
of that the scores of its direct parent
and its gr great-grandparents are
affected right so this is the basic idea
of the rated list uh uh this was what
was defined in the blog uh we went on to
further Define how you would use
filtering over it uh which is uh sorry
scoring yeah so once you have uh every
node descendant score uh so if a set of
uh nodes are serving a sample uh their
descendant scores are not the actual
scores that we use for filtering what we
do is uh we
trickle uh not trickle basically but um
we kind of follow the paths of uh that
node existence in all the sub trees
because a node has many peers and it
would exist in different sub trees uh we
take the level one PE scores which are
the best for that nodes path right so
it's the best part score of that node uh
we use that score we filter out using a
threshold if the threshold does not work
like if it does not filter out any nodes
we switch to an average score uh for the
same and uh um basically because we want
to complete sampling even though uh you
know we cannot filter out using the
rated list so that's um that's average
filtering and after filtering we can
apply different quering strategies which
is we can use random quering strategy we
can use the highest score first we can
use the lowest score first and we can
use the average score first average
score first and lowest score first does
not make a lot of sense in the start but
if you think about it uh parent node can
have and
a perfect uh score because none of its
children are contacted yet right so a
highest score first wouldn't be the most
uh successful strategy in in some cases
right you would want to go for the
average score because you know that
there are some honest nodes balancing
out the uh malicious nodes in that sub
tree right so there are different
quering strategies that you can apply uh
and for the rated list we have defined a
default score of 1.0 which is the
Perfect Score uh which is like
optimistic but it's fine because uh we
do scoring on a per slot basis so uh for
every slot we uh get all the peers uh of
peers and build the tree and uh we start
scoring and we delete the scores for the
next slot we start again right uh
ideally we could what we could do is we
could propagate these scores into the
gossip score uh v1.2 to pist over slots
uh so we could have that kind of a
scoring mechanism as well uh why do we
we use the rated list right uh the main
attack that we are uh going towards is
to defend against a local keyspace uh
flooding attack uh through Cil nodes uh
we also want to reduce the amount of
requests that we sent out uh for
actually completing pure sampling which
can be avoided if we can defunct uh an
entire subtree of malicious nodes uh I
would say that the first slot Tempo
matches very nicely with uh uh with uh
the peer samp sampling requirements uh
because um you can have a honest uh
honest acting malicious nodes which is
default on one particular block right so
the entire objective is to complete
sampling for that particular slot so a
per slot sampling sets the tempo right
uh and lastly it removes dead sub trees
so if there are any inactive nodes they
would already be removed and you could
you could assume that we reach a global
stability Point like where everyone
removes their defun sub
trees uh so then coming uh to the
simulator and the simulator's designed
we had to play around a lot uh because
for graphs uh they're not uh there are
libraries out there but the ones that
are usually used uh are not that uh
optimized so we started with the network
X Library but uh it was really slow at
generating a random graph with a high
degree uh at number of nodes at you know
we use the degree at 50 and the number
of nodes at 10,000 which is like a good
assumption for a p sampling uh Network
right um and we use R Rush work X
because uh as you can see the benchmarks
for uh graph generation is just uh the
best for all the libraries that are
there out there right now uh and for
quering strategy we used all four
quering strategies highest average uh
lowest and actually random as well uh
but we also ref filter nodes with a
different threshold if we cannot
complete sampling uh because our
objective is to complete sampling at the
end of the day right so um uh coming
into the architecture of the simulator a
little bit uh we wanted to modularize
everything so that this simulator can
practically be used for other kinds of
implementations like the rated list in
the future so U having it modularized
make uh gives you a attack framework
because we have written a lot of attacks
right now uh so you could test against
these um in the future uh so yeah
everything's modularized we spec convert
the rated list uh spec into a notep
implementation for all the simulations
I'll give it to
opheim so um yeah that's the
simulator let me just quickly
my said not
working I think it's
working okay um
um um I mean just since the slides are
off let's uh turn the attacks on I
guess so uh yeah I'll uh quickly brief
you guys about uh the sort of attacks we
try to um uh use against our uh
construction
uh okay uh yeah so uh basically what we
tried to do is uh poison the entire uh
network with a lot of randomly picked uh
nodes uh and mark them as malicious uh
the only problem with that is that uh
the conditional probability of finding
uh the parent being malicious uh with
the uh children is almost same as just
randomly picking any uh malicious node
from the network so uh just randomly Mar
marking any uh node uh doesn't make
sense and like there's not a lot of
difference because of the rated list uh
although uh the only attack Vector that
uh that might cause an issue uh would be
uh basically trying to attack on one
local key space and uh that's what we
tried to
simulate uh so yeah basically trying to
Mark and and
subtree uh to a malicious node which uh
then gave us the uh results where we
were getting uh far more better results
in terms of uh rated list uh as compared
to just
randomly yeah there you
go okay uh so yeah uh there's a graph
plot where we uh get to see where
uh the rated list performs much better
than just randomly picking out notes and
trying to sample uh it's in terms of the
number of requests that go out
uh the like for for uh 90% of 70% of uh
civil uh nodes gives us almost double
number of uh requests when doing a
random sampling but in case of rated
list it's like uh um the lower number
so is it okay okay got
it should I
okay yeah so there are graphs uh showing
those results and uh that there's some
future work that need needs to be done
uh we are currently researching on this
new topic uh of trying to you uh score
the parents uh in a probabilistic way
right now it's just averaging out uh so
uh we are trying to take the
probabilities of each and every uh CH
children nerds uh and then uh finding uh
the score for the parent based on those
uh scoring uh for the children so yeah
that's
the uh scoring mechanism we are trying
to work on and uh I mean perfect the gra
oh yeah awesome uh so yeah this is uh
what I was talking about where we were
just randomly poisoning the nodes uh in
the network you don't really see a
massive difference between rated list on
and off uh so as you can see like
there's not a lot of difference but
there's a small difference but uh
still yeah this is where we uh shine is
than just randomly sampling uh from the
network in case of 90% um uh poisoning
so this is where there was a uh entire
subtree marked uh U marked as malicious
and I think I have a
small yeah so the those are the sort of
future work we are trying to do is that
uh since our implementation is still in
like super native uh uh like primitive
stage right now so uh we would like to
know Implement uh a better uh
simulator and
uh yeah I think uh this uh probably sums
up our uh
that's yeah
yeah uh so
um did
we can you go back to the graph yeah so
just want to point out that all these
graphs were done over like uh 100 runs
at different thresholds sorry um so all
all these um uh metrics were plotted
over 100 runs uh with different
thresholds and and different strategies
TR out and for the rated list we picked
the best score with uh in all uh among
all uh different uh strategies right so
best score first average score first and
everything uh where the rated list off
is just plain old naive randomly
sampling from the
network just for probably just for
visual
visualization uh this is the sort of
that the uh big blue ball is basically a
network but then it's all pulled out and
those are the malicious notes yeah so
yeah thanks
guys all right any questions for these
two on the rated list
project uh just trying to understand the
Civil graph that you had you you
injected lots of Cil nodes into the tree
and how do you determine which ones were
sied and which ones weren't or have I
misunderstood um are you asking about
the implementation detail or like uh no
kind of in general so like okay so um a
Cil node uh would basically not reply
for any pure sampling requests coming to
it right uh we can definitely test for
more cases where the attacks are more
nuanced uh as in it can the adversary
can be adaptive where the Cil node only
responds to a certain number of nodes
but not other other nodes and if you
want to actually Eclipse a particular uh
node you would you would flood its local
space and not just reply to that
particular node but reply to every other
node so yeah so you were detecting the
sibles you didn't just assume that they
were sibl right you didn't just say
these ones are malicious you actually
based on their responses you determined
that they
yes
yeah yeah the parent child relationship
that you showed in the graph like how do
you come up with the peer parent child
relationship is it like if this peer
like told you about the other peer then
it's its child yes so it's a it's much
more of a local view rather than a
global view where we it's basically just
like peer exchange where we just ask for
uh peers from particular node like so
first I asked from the node that I am
connected to for their peers and then I
make a list of those peers and ask ask
for peers from them as well I can cap
this uh basically we in the rated list
we have it capped at 100 uh Max children
per node so any node could like randomly
send for every slot randomly or maybe
probabilistically send a certain set of
us is is this a candidate for like
replacing CAD based Discovery it
wouldn't replace cad-based Discovery per
se it would so rated list assumes that
you already have a a P2P network uh so
and for Discovery it's not really like
you would need bootstrap nodes to
actually form this thing so it becomes a
little more complicated over there uh
but yeah but what we are definitely
planning on is like if you think about
it uh per slot scores uh they they make
sense if uh you assume that uh malicious
nodes are honest acting until that slot
but then you can also have malicious
nodes that start acting honest after
that slot and a per slot score wouldn't
really help at that point so uh but the
the good thing is the uh version 1.2
gossip scoring provides like these
application scores that you can inject
into them and they are persisted with DK
parameters and everything so we plan on
just injecting the rated list score into
the uh Gossip V2 v1.2
all right thank you let's give it up for
these guys one more
time all right next up we have a
presentation from gilia and he is going
to be talking about his high performance
ssz implementation
um hello my name is gilia I'm here to
present my work on szb a high
performance ssz implementation in Rust
as well as my work on ssz Arena a
benchmarking suite for the crates in the
rust
ecosystem um my motivation for working
on this project
uh having a chance to work on optimizing
an existing project uh which I haven't
done before and so I learned a lot in
doing this as well
as
seeing what kind of gains were possible
in something like ssz it's not a real
bottleneck in uh clients today it's
still a a fairly simple task but I was
curious about what kind of losses were
present over a long period of time uh if
without an optimized implementation and
so I thought this work was
uh very uh appropriate for the
fellowship because it's I guess lower
priority for client teams but still work
that needs to be done done
so um I'm briefly going to going to talk
about what serialization is and ssz and
go over the ssz
ecosystem as well as my work on the
benchmarking suite and the
implementation finally talking about
performance and next
steps so zooming past these first few
slides serialization is the process of
transforming a data structure
into a common format that uh the
different clients
can agree on which is uh important for
consensus uh normally data structures
can't be transmitted as is because of
different in memory representations and
such ssz is the
serialization scheme on ethereum 2 meor
replace rlp on the execution
layer uh it has a few improvements like
schemas which are a must have in a
performance
system and
merization is the process on which we
generate a short uh digest of the
state um while allowing for updates
without rehashing the entire
State uh this is useful to generate
small proofs of uh the contents inside a
beacon blog for like clients that don't
want to hold too much State
data so uh there are a few ssz
implementations uh in the ecosystem
mainly and go uh fast ssz is uh most
popular one and used in gu today
although Peter also has an imp
implementation out uh in Russ
specifically there's Sigma Prime's
ethereum ssz grandin's crate which isn't
public and is only used internally and
Alex stokes's crate as a zrs uh for the
first half of the fellowship I worked on
a benchmarking suat to evaluate how
these libraries perform against each
other uh most of these crates have tests
on consensus spec tests uh but there's
no real way to see how to perform on
real data like Beacon blocks and Beacon
States
uh so I worked on a uh Library oh not a
library a project called asz Arena which
uh benchmarks the different crates in
the ecosystem and so it evaluates it
on more controll test cases like lists
of uh integers and validators validator
structs and also evaluates it on
blockchain data like Beacon blocks and
Beacon state that I obtained from a uh
Beacon chain
checkpoint so I leverage Criterion for
the this Benchmark Suite which gives us
handy reports but I also use uh
another uh benchmarking Library called
the Devon
which handily gives us allocation stats
so we can see how much memory is being
allocated during these uh encoding and
decoding runs and so this gives us a
robust way to measure
performance now on to the second half of
my work into
Fellowship working on my own
implementation so how does one optimize
ssse
optimizing this uh serialization scheme
and benchmarking is kind of tricky
because most the real bottleneck
or most of the optimization in EN coding
and decoding is simply using a more
optimal data
structure there's a lot of techniques
you can do to optimize this you can lay
out your data in memory so uh it's
aligned to the word
boundaries and the other uh techniques
like zero copy de serialization where
you can simply cast your bytes into your
type that's only really possible when
you have control of the under underlying
data
structures um which I did not have for
this project and there's a lot of
reasons why you might not want to just
rewrite your types with serialization in
mind um Sigma Prime in particular has
done a lot of good work with their
Milhouse crate which um allows for
faster
uh sparse updates of Beacon State and so
it doesn't really make sense to just
change your type that crate just to
speed up serialization it's better to
just think about how to work with uh the
types we have and so being constrained
by
the inability to change the underlying
data structure I opted to minimize
intermediate allocations and so another
a second bottleneck in serialization is
how much memory are you allocating in
between steps to serialize and d
serialize and so here's how I went about
my implementation as a
ZB has two main differences
it uh uses the buff and buff mute
traits this is an abstraction over uh
buffer types
so it encapsulates both vectors and
slices and has the added benefit of
abstracting offset and Counting which
greatly simplifies the implementation so
for context you can outright Define how
to encode and decode certain types but
the SSB package also provides a macro
for automatically generating
implementations for container types
which are like
struts um and so generating this these
implementations is a
hassle we um provide a way to do this
automatically and the implementation for
it is very simple thanks to the buff mut
traits and second we avoid a lot of
intermediate State during the coding
process and minimize any needed
allocations during the the coding steps
uh this reduces the number and size of
memory allocations needed to perform
serialization which is another dominant
cost as I mentioned before um Peter's go
implementation uh SS implementation does
this
and grandine was also another big
inspiration although to note grandine
only works with slices uh the benefit of
using buff and buff mute is being able
to use vectors and any other buffer
implementation you want to provide as
long as you implement the trait which
not quite sure how it's going to be used
just yet but could be
handy and it performs pretty well so
I tested this on beacon block decoding
encoding it's pretty pretty fast on the
the decoding part if you'll consult the
graph I'm not quite sure how visible it
is but um clocking in at around 129 M uh
micros on the decoding part versus 3
milliseconds um in ethereum
ssz and
while the differences aren't as drastic
for uh all types we're getting uh
there's similar levels of performance um
around
coding speed up on the beacon blocks
um so that that was for the beacon
blocks there's still some changes to be
made some fixes to be made for uh Beacon
State encoding and
decoding I know what the I there's a bug
in the implementation I know where it is
I'm going to go fix it but I only found
it like two hours ago so uh didn't
really have time to fix that today uh as
for next steps I want to ship a a
support for merization and Merkel proofs
with generalized indices this is needed
to have a full-fledged as a the
implementation my focus for it is
Fellowship was on performance of
encoding decoding and so I left this for
after the cohort additionally I'd like
to support uh a new trait I call SZ
check which provides uh early input
validation
for to check that a an input conforms to
a certain type this would be useful if
you want to reject malformed inputs
earlier here instead of having a full
decoding step in the hot path of your
application uh and then after that
stable release I want to gear up
for right stable release adding usage
docs and cleaning up anything that needs
to be uh Polished in the
library and then once that's done I want
to work on something I find interesting
but I'm not sure if other projects would
want to use use this but I think it'd be
cool to have support for partial
encoding and uh
decoding for example for large objects
like Beacon State fully decoding can be
very
expensive and so partial decoding would
drastically speed things up especially
if you only need a subfield of your
beacon State and uh re-encoding and
rehashing would work similarly again
with uh the sigma Prime mhouse
implementation they're already
implementing their types with uh sparse
updates in mind and I think uh ssse
could use uh some similar ideas with
regards to sparse
updates I'm a little over time uh I want
to thank my uh Mentor Michael spra from
Sigma Prime who did uh most of the work
I believe on uh ether SSE he's not a def
con uh I think um but if you're watching
this thank you and I also want to thank
uh Josh and Mario for um providing the
opportunity I learned a lot through the
Cort and uh I'm glad I got the chance to
do this
work um any questions that's all for me
yeah all
right any questions about the ssz
library
for uh not right away probably after
stable release I forgot to mention also
that there's no unsafe code in this so
it's uh Michael told me not to use that
uh yeah
um coming soon yep coming soon all right
more time for
gilia all right our final presentation
for the afternoon is from Yash who will
be talking about his explorations into
single slot finality
thank
you okay so hello everyone um uh really
appreciate uh all of you uh coming here
to today to listen to this talk and uh
in this talk basically I want to talk
about the state of dynamic availability
um the state of basically what has been
going on in in the industry and what I
have been you know working on for the
past few months on this uh so basically
I started uh I started the fellowship by
basically understanding um uh like
consensus algorithms and uh looking into
single slot finality but as uh the time
went on I basically focused more on U
Dynamic availability in particular so uh
for starters definitely the question is
uh what is dynamic availability so it is
basically any protocol that is uh live
under what is called dynamic partici
ation which means that basically nodes
can come and go into the protocol and
you know it still works it does it's not
like a static uh a static validator set
where you know the protocol is already
aware of what nodes there are so like
this Dynamic participation a few
examples uh is definitely the Nakamoto
consensus like Bitcoin uh lmd ghost that
is used in ethereum currently and a few
other protocols you know that I will be
talking about today like goldfish and R
lmd ghost um so uh a little bit primer
about lmd ghost if anyone is unaware uh
it is uh it is the folk Choice rule that
is used in ethereum uh it basically uses
uh the weights of WS to decide the tip
of the blockchain so like uh if there is
a block proposal and it wants to propose
a block on top of a previous block it
calculates all the votes uh all the
votes that basically any particular tree
has and and uh so as you can uh see in
this diagram perhaps that the block F
has 20 e has 25 so like when you are at
a you're going to take a step at C
because it seems to be seems to be like
the heavier sub tree and then you can
you know take a step at e and that way
you can you know decide uh which block
to uh which block to choose as a tip now
currently lmd ghost as it exists uh has
some problems it has problems with
Dynamic availability in particular uh
and has a problem with reok so a
particular attack Vector that I want to
talk about is an xand attack so how to
execute it so let's say you have a block
at slot n and uh then it's time for slot
n plus one now the validator here is
malicious and uh it proposes the block
but it does not like releases it into
the network and
uh uh and it has control over a set of
malicious uh attests that attach to the
block privately
so for the majority of the network they
think that the slot is empty so in the
next slot when a new block is proposed
they tend to vote to that slot but as
you can see that uh in the next slot the
uh malicious uh the malicious proposer
then releases uh it to to some of the
network and then some of the testers
might see block n plus1 before the block
n plus2 and then attest to that and then
uh you know this cycle can continue in
um uh for like as many slots as the uh
as the malicious uh malicious proposal
wants if it has like some sort of
control over the network delay and uh
then it can like release um release the
entire thing to the network and like as
more and more slots uh as more and more
slots uh like a part of those slots
attach to this malicious block it gets
more and more weight so like if it then
releases a new block that is uh tied
directly on it you can see that the
weight of the N plus1 block would then
be more than you know the other blocks
so the chain will then reor and that's
how basically honest proposers blocks
will not be included in the canonical
chain because you know the chain has
Reed and that is obviously an
undesirable uh property so how do you
solve this well uh there is uh one
approach and
uh and that approach is basically
goldfish so goldfish uh is uh a protocol
which uh uh as you can see like whenever
slot is someone proposes a block they
vote uh the the testers basically vote
for that block and all those votes are
then you know aggregated and that's what
the tip is like uh what goldfish
basically does different from uh uh lmd
ghost is uh what's called like vot
expiry and vote buffering so vot
buffering is basically you know like
whenever
you whenever you see a your blog
whenever you send a message into the
network uh that you know Network
messages are called like vote
buffering and uh it's also called view
merge so like by view you mean like you
view you view something in your local
View and by merge you mean that you know
you merge what you see uh into what you
think is the current state of the
network because you as an individual
validator you cannot know the state of
the entire network you only know you
know what your local view is by the
nodes that you're connected to the thing
that it introduces its vot expiry and uh
that basically means that it does not it
does not uses the votes of uh it does
not uses the votes from the previous
slot so it only uses the votes that are
casted in this particular slot so you
know previous uh Val validators
basically if they vote on something you
know if uh if there's some sort of uh if
they're trying to withheld V and they uh
they publish it later then it will not
be you know counted so that way you know
there could nobody can you know withhold
hes and they will not be counted they
would be discarded so you don't have
attacks anymore but it does raise uh
another problem and that's the problem
asynchrony so what if there is a network
delay right because
because because if a validator basically
votes for a block and the vote does not
reach in time in the in the particular
slot duration then that ATT testers
votes will not be counted and that is
not the fault of the attest it's just
some network is just a period of
asynchrony right and the Goldfish
protocol would would just not work in
asynchrony so that is a particular
problem that faces another way to think
about the reorg attacks is to think
subcomittees so subsampling or
subcomittees is basically crucial in the
success of these preor tax and because
you know it is easier to like have a
control of you know a small set of
validators and easier to have like
control over the network delayer you
know if there's like a committee because
like a committee has like 30,000
validators but like the entire set of a
theum has like a million so like you
know you cannot have control over the
entire network but you know it's
practical like somewhat possible to have
what about let's say 30,000 no so like
if you just remove subsampling
completely and you know you just have
like the entire million validator set
you know a test in every slot then well
the problem of reor attacks you know
will be solved but uh I mean obviously
that's not you know practically possible
in like 12 or 13 or like so seconds it's
going to take a long time to you know
get all those vots get the um aggregate
the signatures and stuff like
that uh so R rlmd ghost which is like a
a version of this um protocol that I'm
describing uh introduces a concept of
relaxed V expiry and that is basically
uh what what I mean by relaxed vot
expiry is that in goldfish you remember
the uh the votes expire by after just
one single slot but in this you can
basically balance between uh asynchrony
resilience and dynamic availability what
that means is that you can basically uh
Che uh the votes from the most recent
end slots can be considered and the
reason I say that it's a balance is
because is because
uh it's because Dynamic Avail
availability basically means Dynamic
participation like validators can go in
and go out and like if a validator like
is part of the network votes and its
votes is either withheld or like through
Network delay it's stuck and then the
validators goes offline the vote can
still you know affect the protocol which
is not you know a property of dynamic
availability so like uh goldfish is a
perfectly dynamically available protocol
but through rlmd by choosing this
parameter you can balance between both
these factors but U as you see that it
also has a problem of not having
subsampling so is there like an
impossibility between these three
properties uh Rog resilience subsampling
and asynchron resilience and the answer
seems to be yes like uh I didn't get a
chance to work particularly on a formal
proof of this um of this relation but uh
based on you know what I have um studied
and researched so far it seems to be the
case because if you think uh if you
think about it in uh from a view of a of
validator if you do not receive uh an
attestation from another validator there
could be two reasons for that either
that validator is malicious or adversary
and it's withholding the vote or you
know there is just a network delay but
and you as a validator cannot you know
differentiate between the two
so uh so is there a way to basically
combine all these properties together in
a fashion you know that can be useful
for the protocol and uh well a
disclaimer this uh this particular slide
uh needs to like U needs a lot of like
academic review and like a lot of
validation from like people much smarter
than me uh so just take it with a pinch
of salt but uh a possible solution could
be that you know we can use goldfish as
the current uh as our Fork Choice
algorithm and uh we can try to figure
out a way to deterministically identify
a period of asynchrony so let's say that
a committee has 30,000 votes everyone
you know votes for a particular block so
like there are around 30,000 votes per
slot and if let's say the number of
votes you know drops suddenly that could
could be a condition of asynchrony when
uh when the protocol will dissolve the
Committees and shift to an rlmd model
where the where the vote expiry period
would get relaxed it will expand and you
Network and uh you know the votes of
votes from previous networks will then
be allowed to you know be considered
into the into the folk Choice rule then
uh based on how quickly the votes are
coming in we can uh determine if
synchrony has has has been like achieved
again and then we can re aish committees
and the protocol will be back to
normal so uh that is uh roughly it about
the talk I want to uh thank the etherum
foundation and Mario and Josh for giving
me the opportunity to study about this
and I also want to thank uh my mentor
Franchesco who's uh doing consensus
research at ethereum and also Lincoln
who was uh the uh ethereum fellow in the
last cohort uh he also worked on uh
single slot finality so yeah that's it
about my thank you all right thank you
Yash any questions for
Yash so so I'll raise one it's just you
swapping between modes mhm do you think
that that increases the complexity of
your vulnerability and attack surface
analysis because what starts happening
is that a malicious attack can influence
network so you switch from one mode to
the other Etc so that they can take
advantage of of of that
situation Etc
and and amongst other things that just
makes it much harder to to reason about
it and you people forget you know can
forget that actually the Network's not
always operating this Mo in this mode
they need to consider that it changes uh
yeah of course so like uh okay so like
the uh the basically the model that I
talked about is obviously like if you
know we do more research on it it's
going to be incredibly complicated and
switching between like con basically
consensus protocols on the Fly is going
to be very difficult uh the reason for
like uh for my research is just like
exploring what are all the possible
designs you know that can be uh
implemented into the consensus and uh of
course like eum itself is moving more
towards the single slot finality model
so uh it would be interesting to just
see like how Dynamic uh like what are
the properties of dynamic availability
and how you know it could be like
obviously this is not something that I
would you know proposed to actually put
into ethereum but like it's still
interesting to like research about
it all right thanks Yos thank you guys
one more
all right that concludes the
presentation portion we will have a
panel with a few of our previous fellows
who are now uh working either on client
teams or in other uh granted positions
so give us just a minute or two to set
up for that and we will get started
k
w
good
hey okay hey every everyone we'll be
starting our panel in 1 minute if you
can join me um my follow
panelists um yeah yeah
okay okay I guess we can start we'll
already a bit late uh but we've managed
finally to get all the all the panelists
here so yeah um first of all thank you
so much for being here uh all the all
the fellows on the panel and all the
Fells and attendees and mentors who
mentioned to make it to the end of the
EPF day of
EPF day with a special panel discussion
and I call it the E panel discussion
because it's uh you know ethereum but
it's also uh Echo Ethan and ano um uh
completely randomly chosen follows of
course to join us and uh so I I chose
you guys I invited you guys uh because
all of you um have been in previous
cohorts of the fellowship and today uh
you became full-time contributors you
became uh working with uh client teams
or actually like um so I will start with
quick intro and I will kind of do your
uh intro you can add if you want
something because uh I want to move to
the questions but uh so we have uh we
have here uh three people who have uh uh
I believe are representing all kinds of
possibilities that EPF can give you uh
because we talk a lot about client teams
and and the core contributors and so on
but not everybody from EPF is going to
end up working working in a client team
and uh there there are other kinds of um
uh features for you and I will start
with uh with the lady here Eno uh who's
been um uh working on uh dep packaging
uh as a part of um uh created a
verifiable deterministic builds for
ethereum clients and uh has been working
on it uh for more than a year now uh as
as a as a grantee so continues her uh
EPF work um on a grand uh and working on
something which is you know ethereum
tool L but not direct to the clients so
uh great example of like how um how how
uh people from EPF can work outside of
clients we have Eon who is on the other
side directly in the client working
full-time in
Lighthouse uh as a as a core developer
and we have Eko who is a researcher uh
in ARG or yeah applied research group in
in ethereum foundation so we have also
representative of a research uh flow so
yeah all kinds of all kinds of
backgrounds and uh and uh I don't want
to actually like talk really about your
EPF experience itself because uh uh you
can actually find it right if you go to
cohort 3 cohort four four repository all
of uh all of the experience all of the
work done by these people is there it's
open uh it's uh you can you can read
their story for week by week basically
so uh we don't need to dive into that
but I want to ask you folks um uh uh
looking even before EPF before you uh
before you started diving into ethereum
diving into uh the fellowship uh what
was it like how did you uh what did you
think at the time what brought you to
the EPF what was your like initial
motivation to start working on this what
what yeah uh What uh interested you uh
what was the initial push to get into
all of this whether EPF or ethereum yeah
um see whoever wants to
start um yeah so um I mean I got into
ethereum or blockchain originally uh
because I saw my friends use a um they
took out a like collateralized Loan in
like like a minute and we're moving
money around and I'm like what is this
like how does this work I don't
understand this um and then they got
liquidated like a week later and I'm
like oh this is really cool like I want
to learn more about it um and then you
know fast forward a couple years I got a
job in like web 3 um doing like you know
app development uh and I came across the
EPF actually during cohort 3 um it had
already started so I was just kind of
like peeking around seeing what's going
on and I think the the merge was
happening at the same time and when the
merge happened like one of the main
things that I noticed was like how it
just worked like there was no no issues
no bugs uh which is kind of like a crazy
concept I don't know if you guys play
like video games like I used to play
World of Warcraft a lot and every time
there was a new expansion the servers
would crash like day one you know there
was always an issue and for this like
huge distributed system that you know
multiple teams worked on um previously I
didn't even know that there was multiple
client imp implementations either which
like kind of tripped me out as well um
and so you know seeing all that um just
like really impressed me and I wanted to
learn more about that world and then you
know cohort 4 came along I had already
made a couple small PRS to Lighthouse um
and interviewed and yeah got in got in
that way so so you know you know that
ethereum was created because of World of
Warcraft yeah yeah yeah yeah I mean
literally yeah it's it still keeps
producing Cordes it's amazing yeah um
yeah yeah think any yeah any Co EO yeah
whatever um so what got me into
blockchains uh it was a few years back
whenever I was in the tail end of
college and I was not really like was
super lost and all of a sudden like Co
hit and I kind of saw a lot of instit
ions and like systems that were that
were super fragile and like all of a
sudden broke out of nowhere even though
there was like this veil of legitimacy
that was kind of shrouded amongst all
these things that ended up breaking um
and I saw technology as like in
blockchains as like a way to kind of
help build more robust systems to
coordinate humanity and thought it was
really cool and so I just started
tinkering with um with like I was
building some some light client that
ended up like it was ridiculous looking
back but like it it kind of got me got
me in the door I met a client developer
he told me about uh the the protocol
fellowship and so uh yeah that's what
happened and I applied and you all said
yes so thanks uh so Word of Mouth helps
to run Fellowship yeah all the time yeah
dude yeah and I think like a big thing
was uh interacting interacting online um
with with people in the ethereum uh
Discord and like you can you can find
these people online and a lot of them
are like really kind and are super
excited to help anybody who's interested
so yeah um so for me is um I'm I'm
living in Thailand so I moved here like
um decade ago and I live in changai
which is I don't know if how many of you
went there to do a short visit it's
basically a crypto City so Bitcoin is a
very very um heavy Dominion there and I
was always like oh I don't really want
to get into this because it's um it's
just all these crypto BS and they just
want to do with all this um new token
and everything so I was like I no no I'm
opposing to but um near Co um um I
started working in a co-working place
which is like a blockchain space and
there was this guy who was doing a Chun
Mups and uh uh he was hosting a
blockchain um U Meetup and um um my
friend just convinced me to just go once
um so I went and I was like U he was
showcasing Unis Swap and stuff like that
and I was like wow these are really
really cool stuff like um I was like the
first time that i' actually see projects
that trying to make an impact and at
that time we don't didn't have the word
for refy because that came couple of
years later but I was like oh this is
like people are actually trying to build
something that it's very decentralized
but also like uh also makes money for
the people so it's like for me this was
the initial sub that made me in wanted
to um be a programmer so it was like I
was really said for a very very long
time that I couldn't really find all of
these projects and I was here out of
nowhere uh just after the co and yeah so
that's that's where kind of my journey
started and slowly slowly um um I get it
more and more invol than the the EPF
last year which is like also was crazy
because I applied like I think a week or
was like I'm not getting in I'm just
applying so I actually wa to in C five
or something like that and oh I got to
accept it so yeah that's a story for me
so where did you learn about EPF what
what what uh yeah where did you learn
about it where was it coming from U EPF
oh so I actually know one of the uh
previous follows from cohort 2 so that's
how right so again word of M and
Community it's like follows bringing
more follows
um please bring more more follows next
year yeah uh yeah thank you so much
everyone uh yeah that's lovely so yeah
that was uh before the cohort started
and now I'm going to skip the cohort
because again it's all on GitHub you can
read the experience and we discussed
like how it helped you and so on at the
EPF days before but now I wonder like
what happened for you since then so it's
been like one year for you folks one and
a half year for Echo since the since the
Cort ended uh what is your biggest
Revelation since then what was some uh
highlights some biggest progress you've
made some yeah some some Revelation you
had since uh working without a training
wheels of EPF basically
yeah um no okay so stays here um so I'm
still working on the project and I'm
working under the EPF so I'm um trying
to simplify noing but also providing
packages for Linux and um it's a pretty
complicated project so there's a lot of
moving Parts I've I was think um I got a
comment that it's um I um um my project
is um um to to I I don't know I don't
remember the word but it's it's too much
so it should have been just uh Sly the
smaller one either way um one of the
stuff is like just keep going and keep
doing and things start to work out um
and uh it just takes time more time than
um than you would originally s but it's
still um it's just just just be patient
and persistent and things going to work
out yeah for me that
was yeah I mean um one of the biggest
highlights uh so far being a cev was uh
being able to go to uh one of the
interop events so it was about I don't
know six months ago or something we all
uh all the client teams went to Kenya
and we spent a week um kind of at this
Resort and we all just kind of were
stuck in a room together and worked for
a week um and for me it was a really
cool experience one I got to meet a lot
of my team members that i' had been
working remotely with and two I got to
interface with you know researchers
other client devs people that have been
working as core devs for years uh people
that I you know respect that I look up
to and I was sitting in the same room
with them um you know having
conversations with them uh I probably
learned more that week than I'd ever
have about blockchain any other period
of time um and it was such a cool
experience I tell everyone I felt like I
was the the dumbest person in the room
but in a like in a really good way like
I felt like I was able to absorb so much
much um and I felt just like so lucky to
be there um it was it was really really
cool um I guess the biggest thing that I
learned like going off of this like
dumbest person in the room thing is that
I like I got really scared a lot of the
times whenever I was interacting with
these uh core developers um and shied
away more than I like looking back I
wish I didn't shy away so much like I
was really I didn't ask uh questions
that I had uh like I was thinking
because I was like ah what if someone
thinks I'm I'm stupid or something like
that and like um that's recently started
to change but I still feel that like
part of me that's fearful and um I don't
know you can't like don't operate out of
fear because that sucks like it's
really not a good way to to like go
through these go through why um yeah uh
yeah thank you so much uh I mean it's
all great advice uh whether it's like
working hard whether it's you know don't
uh be worried to ask questions uh yeah
that's amazing uh because that was kind
of my next question uh asking about some
advice or maybe like something from the
perspective at a time when you were
doing or finishing the cohort uh what is
something uh you wish you knew at a time
what is some advice you would give to
yourself at a time when you were still
uh just just starting so that would make
it easier for you um uh yeah with
considering your current experience yeah
sure um yeah don't operate out of fear
um that's definitely a big one and I
think like trying to keep the north star
or whatever is like driving you forward
your own internal North Star I know this
is like a meme that's been going on but
um have your own uh and yeah like don't
forget it whenever you go super deep
into to the technical like the weeds of
things because it's really easy or at
least for me to like go super deep into
something and forget where I was and
like if you kind of forget where you
were then it's it's easy to to not
remember 3 months down the road like oh
yeah what was this all tying back
towards
so yeah I think one thing I told myself
towards the end of the EPF is like
regardless of you know what happens the
the four or five months that I spent um
I were so like valuable and I learned so
much I became a better developer that
regardless of like you know what ends up
out of it like I know like personally I
grew a lot as a person as an engineer um
and I could take that anywhere else even
if it's not in ethereum um and on the
other hand I also was like dedicated to
being like very persistent so I kind of
told myself like I'm going to keep
opening PRS at lith house until they
have to hire me like you know like I
wasn't going to give up um and so those
were kind of the two things that I had
in my head and they made me feel you
know I I wasn't like too stressed about
it because I knew that regardless I
gained so much out of it um that it
didn't matter what the what was going to
end up happening in the
end um I think I I I have this question
before so um I I think the same answer
uh as I said like a year ago is like uh
just talk more with the people uh like I
feel I didn't really interact so much
with the people and the people are so
helpful like everyone's like from the EF
and all of the teams and so on and so on
and maybe they don't have the time to
reply to every PR or every question that
you have because they are busy and they
have I don't know like I think they have
like a tons of PR there um or cash
issues so it's it just takes time for
people to reply or just even understand
I think I opened the pr on one of the
client and they still working on it
because they are just so much overhand
with all the task and uh all the road
map so uh just be really patient and and
and interact more and uh and um and then
don't work in isolation because for me
it's like oh I can just do it alone and
I can do everything and that's true but
I think it's when I get the input I I
can get out of this rubbit holes that
sometimes I'm I'm really really um um
dig myself into because it's just
somebody just says to me something in
like one sentence and like saves me two
weeks of work so that happens a lot so
that that's just PR good for me yeah
yeah awesome thank you very much ano
yeah uh yeah it's all amazing advice uh
very inspiring all of you guys I I hope
to use you to inspire and motivate the
current follows Ed because it's U I
think it's maybe a bit um tough time at
the end of the cohort when you kind of
don't know what's coming next right uh
yeah uh I also wanted to ask um about
that like what was it like during this
transition period um before I get there
I want to check with there any questions
from the audience I want to also give
you chance to ask questions and I can
just you know
continue uh ask my stuff but I want you
are here with these people I want to
give you the opportunity same like the
office hours right don't be shy classic
right uh uh Dan is missing here to drive
it oh we still have Kya she's she's the
runner up uh yeah we have a question
already um yeah if you have questions
please feel free to raise your hand
we'll give you opportunity uh also there
are three people three very different
expertise covering like most of the
uh EPF core protocol related stuff so uh
feel free to direct on the individuals
or the whole panel uh so if you have
questions yeah always feel free to ask
um and if there are any or or not I can
maybe continue but yeah okay K go ahead
yeah I will I will I want to save mine
because we still have time so yes thank
you for coming I just wonder how long
was y
cohorts how how how long how long how
long was the cohort the cohort yeah four
four month yeah the same four months it
was before it was used to be four months
and uh it was running uh
uh Echo is from the cohort 3 which was
the first one that we did with Josh so
also for us it was a new thing running
from like uh November till February um
uh 2020 uh uh 2 till 2023 um and then in
ano and Ean um participated yeah so yeah
thank you um yeah yeah so so yeah uh
chea uh hello I'm J V and I just
wondering uh what's working exp parents
as working for AUM and for other
companies
yeah so what's what's the working
experience different from working forum
and for us
companies sorry s um so uh employes for
yes Foundation is very stable and uh
yeah but for us must
be yeah yeah because folks again you
have very different experience so maybe
like what is it like working on a grand
working for a client team working in EF
and maybe if you have other experience
from like corporate kind of normal
businesses like if you can compareed it
yeah yeah I mean I've worked like uh in
the healthcare industry e-commerce and
also like web 3 app layer and um those
were already always like very like
product focused very
um I guess like uh very like um we' had
like a product manager like we need this
feature or if that site is down like you
better wake up on a Saturday and like do
this um and so working on like open
source software um like lighthouse um
has been kind of like the opposite where
um you um you like obviously there's
things like Petra like the you know the
upcoming upgrades that you know we kind
of have to do as a client team but we
also have a chance to like explore um
you know maybe ideas that we have or
work on things that interest us um and
everyone on the team is also like very
passionate about ethereum like you know
when I worked in e-commerce I wasn't
passionate about e-commerce you know
like um so it's really cool to like work
in a field that you also like care about
in your personal time um and
um yeah I get and and then also like not
just that I get to interface with other
client teams and other researchers that
aren't even part of our company um but
we're all like collaborating together uh
which is very weird I I had this
discussion with someone about um you
know the different client teams like
let's say Lighthouse and prism and he
says is there competition between
Lighthouse and prism and I said no like
you know we care about client diversity
like you know we want you know a healthy
uh diverse Network um and they just
couldn't wrap their head around it
they're like what do you mean like you
guys like don't you know compete or hate
each other or whatever and it's like no
we literally collaborate like as I said
we spent a week in Kenya like talking uh
sharing ideas um learning from you know
each other's like implementations or
whatever um so it's it's a totally
different Paradigm than I was used to in
you know other jobs that I had
um so I didn't have any sort of real job
uh before before working at the EF and
and that's
definitely it kind of has it has its
pros and its cons but um yeah there's
there's like a lot of room for you to
kind of explore what it is that you want
to do as a researcher at least um so
there was not
really there wasn't much structure at
all and you just kind of go off and
figure out what's interesting to you and
you explore that and you it's on you to
like reach out to others and try to
learn from others and it's on you to
like hold yourself like to set goals for
yourself and like to make sure that
you're staying on the right track and
progressing the concept that you're
exploring or whatever so that was very
interesting and I think there's some
organizational changes happening so it's
a little
less I think it's we're we're moving
from this like super free formed
research group to kind of like slightly
more directed so that's changing but
yeah that's how it was for
me um all right so I think um I have
quite a diverse experience so I was
working for both corporations and for
startups also for web we free and non
fre basball and this is like uh
basically kind of like my own company in
a way because um I'm seeing my own
company because I have to do the
development I have to do the project
management I have to do the marketing I
have to do the community building so
it's like it's a lot and it's like I I
like it's it's really like wearing a hat
like like which hat you are wearing um
in different days and it it's just I
cannot do like coding in on the same day
and do marketing so then know I'm trying
to to like make each day difference so
when I'm just read just on Twitter or
I'm I'm still not though um I'm trying
my my best um so it's um I think it's
it's very challenging um to do a project
like this kind of side that I'm I'm
actually started under EPF and um I'm
still in one in many ways I feel l so
I'm really happy that to be here in
Devcon and meet people and um get
feedback and advice or see how people
are actually doing um uh different
projects um I think it's really
rewarding uh so it's definitely
something that I really love to do it's
just tons of tons of work and uh if you
know something breaks and um um I'm
actually supporting all the clients
which is like 10 at the moment and we'll
be gr with grand D will be 11 so it's uh
every time they just push an update like
on Saturday like
Lighthouse I have to just go and then do
the release so it's like okay um I mean
it's on them it's like sure why not um
but it's it's like I have all the
processes done so I don't have to be
there and do all this so it's it's it's
U it's really really kind of like
building up all of this so I can free up
my um uh time and do um development work
which is what actually matters and um
it's challenging but also very rewarding
I think yeah thank you so much ni I mean
it's it's uh yeah very hard task to
manage your own project independently
but uh yeah a huge respect for all uh
working like this and you guys are
pushing releases during Defcon like
you're you're working
like how is it even possible anyway yeah
thank you so much everyone um yeah I'll
audience like feel free to yeah I see
some hands there yeah sorry good
afternoon everyone so my question is um
I had a project and it had something to
do with Lighthouse so um I find it
difficult to navigate through Lighthouse
so I really want to understand how do
you guys maintain the code base because
the code is kind of like much compared
to grinding and most it's more like like
there's a file that has like up to 6,000
lines of code like
whenever I look at the code base I I I
find it difficult to digest like how do
you as a developer how do you navigate
through so much code base how do you
maintain how do you test and all of that
yeah I remember when I when I first
looked at Lighthouse I also um was sort
of new to rust and I was pretty lost and
overwhelmed and I think that happens a
lot like in general when you look at uh
new code bases like when you started a
new job um you know you have to learn
you know the infrastructure the business
logic and so what I really tried to do
is like start small so I picked like the
easiest PR I could find I think it was
like to like update a log message like
literally like a on line it was just
like changing like a string or something
it was really silly um and then I got I
took another PR that was a little more
complicated and just started building on
things so instead of like trying to do
something big at first which would have
been like really really overwhelming for
me personally uh because the way I learn
you know everyone learns differently but
the way I learn is like by doing things
um I just started with like the simplest
PR you know issues I could find um and
built myself up from there and
Lighthouse is a really complex code base
um there's a lot going on there I mean
I've been working at Lighthouse for
almost a year and there's you know
things in the codebase that and that um
I'm not you know super familiar with um
and I'm you know learning to this day
and I'm sure even a year from now I'll
be learning um so yeah there's there's a
lot there um and you know we are on
Discord so if you have questions um you
can message us on Discord we're super
responsive um and we can try to help
um but yeah it's definitely a lot it's
not uh I I also had trouble at first for
sure Claude
helps like it it genuinely it's getting
good now but like you can just highlight
everything within within the repository
and if you have a question be if you
have a question highlight the repository
and then ask the question like all right
how do I like where do I start and these
days it's pretty good at like it kind of
knows the protocol pretty damn well and
like understands rust the language
pretty damn well and it can it can
definitely point you in the right
direction that's an interesting heck
yeah and any feel free to add like
navigating or like maybe from to reframe
it from your perspective because uh you
were so not diving into like existing
big code base but you were starting from
zero but a very complex project so when
you are starting from zero where do you
start building something a have to patch
kind of a few clients on my project so
because um not all of the clients are um
allowing for reproducible boots so it's
like five or six language that I have to
be in some ways familiar with so it's
just basically they put the build IDE a
year some sometimes something else so
it's a it's break so it's I have to
redict through why it's not working so
it's it's was really challenging to to
like yeah but I think it can command
that um and also it C it it's just start
small and keep building on it and small
things done over time it's like have
like like people like always say when
you save money do the compounding
interest and this in 10 years you know
like you will get rich and I think
people don't use that enough for
development purposes because I think
that's how it works and I think that's
why atan got hired by lighthous as well
because he started very small and keep
building up onto and it became very big
in in the end so so I think it just
start small and do and definitely do PRS
like like I think SM PR start with smart
PRS and uh you build it
up yeah it's awesome that's a great
advice as well Compound Effect yeah um
any more questions anything uh from the
audience feel free to ask uh generally
or yeah
gilia uh thank you um I had a question
for you uh you said you working on a
research project and and um I was
curious about how you
went about maybe if you got stuck in
like rabbit holes or anything like that
uh how you kind of got yourself out of
those I have a bit of research
experience and often I found myself just
like spending a long time just thinking
instead of doing as opposed to like when
I code and I can kind of see where I'm
going there's any progress and stuff
like that where your mentors any help um
for I
guess um what what I'm looking
for getting you unstuck and having like
reaching milestones and
stuff yeah definitely
um I think the big thing is like
starting with a goal like and and don't
forget that goal like if you have a
question don't forget the question um
and then like
making making a game plan based off of
that like these guys y all the at the
EPF um you all had like weekly updates
that you had to do and it forced me to
to write and like to like develop the I
like help me keep track of the goal and
to yeah like over time see if I am
veering off and not answering
the question or not answering the sub
question that leads back to the main
question I think like just visualizing
and remembering what the main goal is
and then also like I was lucky or I am
lucky to to be with Alex and Matt and
like you can ask these guys I don't know
if you can find somebody who who knows
about the topic uh and has research
experience they can definitely help
guide you um and like set the right
goals into to make the right game plan
yeah but writing writing out helps
because like otherwise it's really easy
to just read stuff and to think you
understand what's going on but then like
someone asked you a question where you
have to answer based on yeah like if
you're not writing you only think you're
thinking kind of
thing all right thank
you awesome uh yeah thank you so much EO
um yeah we have last few minutes to go
and um I would um yeah I would uh uh
maybe close the panel with one more
question uh because I'm wondering like
um so there is a there is a big
difference between the EPF and now being
just you know without the training
wheels and I think one part of the the
training or in EPF or one thing that we
did which you mentioned the weekly
updates many people say that like having
this routine and the community which
pushes you motivates you to build stuff
right uh so I'm wondering like how this
how does this change for you now what is
what keeps you going what keeps you
motivated what keeps you uh productive
in a way whether you whether it's a
routine whether it's some development
process or like yeah what what keeps you
going guys
yeah um for a while this past year I
kind of fell off and like got in a
really bad spot uh but having like
people around you in real life at least
for me helps a lot and like if you know
the people that you're working with and
you believe in the thing that you're
trying to to build like the world
computer and all the sub components of
the world computer like yeah it
definitely helps you stay motivated um
yeah yeah don't don't get like so stuck
in a rabbit hole or in your head that
you you feel like you can't uh like talk
to your to your people uh
yeah um yeah I think uh for me I'm
definitely like a creature of habit and
I think like motiv you're not going to
always like be motivated every day but
um what I try to do is like to be
consistent So like um I have my routine
I wake up you know at a reasonable time
I have you know get a couple hours of
work done go to Jiu-Jitsu get some
exercise come back and get some more
work done and like my wife always makes
fun of me because like no matter where I
am like I'm in Thailand right now and
I'm still going to Jiu-Jitsu and she's
like like annoyed you're like you're in
Thailand like let's go see temples and
travel but like I like doing you know
the same thing wherever I'm at because
it gives me like a little bit of
structure um and also like I get to work
like with really talented team members
um and I've learned so much like so much
these last you know 10 or 11 months um
that you know um I feel so like uh like
privileged to be able to work with those
people um and learn from them that like
how can I not like wake up every day and
you know do my job you know like so many
other people don't have the opportunity
that I have um and so I'm really like
thankful for it um and yeah that's what
you know gets me excited I guess most
days um yeah um habits are definitely
helpful um for me um uh I I have uh the
big Vision Insight so I really want to
simplify node running and I I just
shipped the Alpha version which is
just like a month ago or something so I
really want to see the project going and
it's it's not in in a state where I I
could be proud of it and I'm really
waiting for that feeling that okay I'm
I'm proud of this that I did this
project so for for that point I would
say I'm pretty much motivated after that
I think what I'm is going to keep me
going is the community so if I start
building the community right now and
people start um requesting different
stuff on on the repository or start
using the project and then that's that's
okay this project is useful and people
want to use it so um that that's are the
two thing that I have in mind to make
myself motivated in the long
term uh yeah thank you so much everyone
uh that's that's a really valuable
advice because you know I mean we are
it's U I want to say like it's yeah uh
one more question we are we are out of
typ so I'm sorry but we will be here so
we can chat afterwards but now I uh need
to cut it off because we're already
after 5 and get to close it here but I I
wanted to uh say a huge thank you
because yeah very valuable advice uh uh
about the um just the motivation that
you just said I find very important
because um it's not even a marathon it's
a it's it's it's just like it's an
infinite Garden right it's this Garden
which just never stops growing you still
need to trim it you still need to go uh
it's you uh people like you are here for
the long run and uh for the fellows here
like uh it's been 5 months and I know
it's been intensive but now is where the
real work starts and now it's uh now
it's when uh the actual part where uh uh
the the career opportunity of being uh
in this uh core Community um starts and
uh uh yeah uh many thanks for you to
inspiring these people and helping them
out uh you are great example of uh of uh
I think success in in this kind of
community but also uh you may notice
that I uh run this panel with like no
technical question it's just like about
our internal motivations and struggles
and feelings and whatnot not because
like I believe that people here can
relate to your experience because you
really very similar thing but uh I want
to emphasize all of these three people
are technical Geniuses they are just
amazing in what they do so uh I didn't
take the opportunity to talk about some
cool technical staff and I know there is
a lot of technical talks that have con
but also please afterwards uh feel free
to reach out to any of them uh ask them
to Mentor you maybe or so uh so uh just
wanted to wanted to highlight that but
uh yeah um I think that's I think that's
it so thank you so much everyone again
thank you so much uh Eko Ethan Ando and
all of the follows yeah thank you very
um the E
pedal yeah yeah that's not going to
happen again uh uh e pedal it was very
special So yeah thank thanks guys and uh
uh follows if you if you don't mind uh
the people from the cohort 5 you would
like to take a photo together uh as a
cohort 5 uh so just yeah stick around
for a moment and then at 7 uh at 7:00
p.m.
uh we have a dinner in The Blue
Elephant restaurant near the hotel folks
you are invited as well uh mentors and
and follows you that also invited uh if
you would like to join us yeah cheers
